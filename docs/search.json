[{"path":"/LICENSE.html","id":null,"dir":"","previous_headings":"","what":"Apache License","title":"Apache License","text":"Version 2.0, January 2004 <http://www.apache.org/licenses/>","code":""},{"path":[]},{"path":"/LICENSE.html","id":"id_1-definitions","dir":"","previous_headings":"Terms and Conditions for use, reproduction, and distribution","what":"1. Definitions","title":"Apache License","text":"“License” shall mean terms conditions use, reproduction, distribution defined Sections 1 9 document. “Licensor” shall mean copyright owner entity authorized copyright owner granting License. “Legal Entity” shall mean union acting entity entities control, controlled , common control entity. purposes definition, “control” means () power, direct indirect, cause direction management entity, whether contract otherwise, (ii) ownership fifty percent (50%) outstanding shares, (iii) beneficial ownership entity. “” (“”) shall mean individual Legal Entity exercising permissions granted License. “Source” form shall mean preferred form making modifications, including limited software source code, documentation source, configuration files. “Object” form shall mean form resulting mechanical transformation translation Source form, including limited compiled object code, generated documentation, conversions media types. “Work” shall mean work authorship, whether Source Object form, made available License, indicated copyright notice included attached work (example provided Appendix ). “Derivative Works” shall mean work, whether Source Object form, based (derived ) Work editorial revisions, annotations, elaborations, modifications represent, whole, original work authorship. purposes License, Derivative Works shall include works remain separable , merely link (bind name) interfaces , Work Derivative Works thereof. “Contribution” shall mean work authorship, including original version Work modifications additions Work Derivative Works thereof, intentionally submitted Licensor inclusion Work copyright owner individual Legal Entity authorized submit behalf copyright owner. purposes definition, “submitted” means form electronic, verbal, written communication sent Licensor representatives, including limited communication electronic mailing lists, source code control systems, issue tracking systems managed , behalf , Licensor purpose discussing improving Work, excluding communication conspicuously marked otherwise designated writing copyright owner “Contribution.” “Contributor” shall mean Licensor individual Legal Entity behalf Contribution received Licensor subsequently incorporated within Work.","code":""},{"path":"/LICENSE.html","id":"id_2-grant-of-copyright-license","dir":"","previous_headings":"Terms and Conditions for use, reproduction, and distribution","what":"2. Grant of Copyright License","title":"Apache License","text":"Subject terms conditions License, Contributor hereby grants perpetual, worldwide, non-exclusive, -charge, royalty-free, irrevocable copyright license reproduce, prepare Derivative Works , publicly display, publicly perform, sublicense, distribute Work Derivative Works Source Object form.","code":""},{"path":"/LICENSE.html","id":"id_3-grant-of-patent-license","dir":"","previous_headings":"Terms and Conditions for use, reproduction, and distribution","what":"3. Grant of Patent License","title":"Apache License","text":"Subject terms conditions License, Contributor hereby grants perpetual, worldwide, non-exclusive, -charge, royalty-free, irrevocable (except stated section) patent license make, made, use, offer sell, sell, import, otherwise transfer Work, license applies patent claims licensable Contributor necessarily infringed Contribution(s) alone combination Contribution(s) Work Contribution(s) submitted. institute patent litigation entity (including cross-claim counterclaim lawsuit) alleging Work Contribution incorporated within Work constitutes direct contributory patent infringement, patent licenses granted License Work shall terminate date litigation filed.","code":""},{"path":"/LICENSE.html","id":"id_4-redistribution","dir":"","previous_headings":"Terms and Conditions for use, reproduction, and distribution","what":"4. Redistribution","title":"Apache License","text":"may reproduce distribute copies Work Derivative Works thereof medium, without modifications, Source Object form, provided meet following conditions: () must give recipients Work Derivative Works copy License; (b) must cause modified files carry prominent notices stating changed files; (c) must retain, Source form Derivative Works distribute, copyright, patent, trademark, attribution notices Source form Work, excluding notices pertain part Derivative Works; (d) Work includes “NOTICE” text file part distribution, Derivative Works distribute must include readable copy attribution notices contained within NOTICE file, excluding notices pertain part Derivative Works, least one following places: within NOTICE text file distributed part Derivative Works; within Source form documentation, provided along Derivative Works; , within display generated Derivative Works, wherever third-party notices normally appear. contents NOTICE file informational purposes modify License. may add attribution notices within Derivative Works distribute, alongside addendum NOTICE text Work, provided additional attribution notices construed modifying License. may add copyright statement modifications may provide additional different license terms conditions use, reproduction, distribution modifications, Derivative Works whole, provided use, reproduction, distribution Work otherwise complies conditions stated License.","code":""},{"path":"/LICENSE.html","id":"id_5-submission-of-contributions","dir":"","previous_headings":"Terms and Conditions for use, reproduction, and distribution","what":"5. Submission of Contributions","title":"Apache License","text":"Unless explicitly state otherwise, Contribution intentionally submitted inclusion Work Licensor shall terms conditions License, without additional terms conditions. Notwithstanding , nothing herein shall supersede modify terms separate license agreement may executed Licensor regarding Contributions.","code":""},{"path":"/LICENSE.html","id":"id_6-trademarks","dir":"","previous_headings":"Terms and Conditions for use, reproduction, and distribution","what":"6. Trademarks","title":"Apache License","text":"License grant permission use trade names, trademarks, service marks, product names Licensor, except required reasonable customary use describing origin Work reproducing content NOTICE file.","code":""},{"path":"/LICENSE.html","id":"id_7-disclaimer-of-warranty","dir":"","previous_headings":"Terms and Conditions for use, reproduction, and distribution","what":"7. Disclaimer of Warranty","title":"Apache License","text":"Unless required applicable law agreed writing, Licensor provides Work (Contributor provides Contributions) “” BASIS, WITHOUT WARRANTIES CONDITIONS KIND, either express implied, including, without limitation, warranties conditions TITLE, NON-INFRINGEMENT, MERCHANTABILITY, FITNESS PARTICULAR PURPOSE. solely responsible determining appropriateness using redistributing Work assume risks associated exercise permissions License.","code":""},{"path":"/LICENSE.html","id":"id_8-limitation-of-liability","dir":"","previous_headings":"Terms and Conditions for use, reproduction, and distribution","what":"8. Limitation of Liability","title":"Apache License","text":"event legal theory, whether tort (including negligence), contract, otherwise, unless required applicable law (deliberate grossly negligent acts) agreed writing, shall Contributor liable damages, including direct, indirect, special, incidental, consequential damages character arising result License use inability use Work (including limited damages loss goodwill, work stoppage, computer failure malfunction, commercial damages losses), even Contributor advised possibility damages.","code":""},{"path":"/LICENSE.html","id":"id_9-accepting-warranty-or-additional-liability","dir":"","previous_headings":"Terms and Conditions for use, reproduction, and distribution","what":"9. Accepting Warranty or Additional Liability","title":"Apache License","text":"redistributing Work Derivative Works thereof, may choose offer, charge fee , acceptance support, warranty, indemnity, liability obligations /rights consistent License. However, accepting obligations, may act behalf sole responsibility, behalf Contributor, agree indemnify, defend, hold Contributor harmless liability incurred , claims asserted , Contributor reason accepting warranty additional liability. END TERMS CONDITIONS","code":""},{"path":"/LICENSE.html","id":"appendix-how-to-apply-the-apache-license-to-your-work","dir":"","previous_headings":"","what":"APPENDIX: How to apply the Apache License to your work","title":"Apache License","text":"apply Apache License work, attach following boilerplate notice, fields enclosed brackets [] replaced identifying information. (Don’t include brackets!) text enclosed appropriate comment syntax file format. also recommend file class name description purpose included “printed page” copyright notice easier identification within third-party archives.","code":"Copyright [yyyy] [name of copyright owner]  Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License. You may obtain a copy of the License at    http://www.apache.org/licenses/LICENSE-2.0  Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License."},{"path":"/authors.html","id":null,"dir":"","previous_headings":"","what":"Authors","title":"Authors and Citation","text":"Andrew Singleton. Author, maintainer.","code":""},{"path":"/authors.html","id":"citation","dir":"","previous_headings":"","what":"Citation","title":"Authors and Citation","text":"Singleton (2023). harpCore: Core functions methods harp ecosystem. R package version 0.2.0, https://github.com/andrew-MET/harpCore.","code":"@Manual{,   title = {harpCore: Core functions and methods for the harp ecosystem},   author = {Andrew Singleton},   year = {2023},   note = {R package version 0.2.0},   url = {https://github.com/andrew-MET/harpCore}, }"},{"path":"/index.html","id":"harpcore-","dir":"","previous_headings":"","what":"Core functions and methods for the harp ecosystem","title":"Core functions and methods for the harp ecosystem","text":"Core functions methods harp ecosystem package underlies everything harp. defines classes, generics methods used harp packages, well functions needed one harp packages. important classes : geolist - list 2d georeferenced fields. harp_df (subclasses) - Data frames containing harp data. harp_list - list harp_df data frames. Methods defined dplyr verbs. addition, functions handling date-times, geographic transformations gridded data, data frame manipulations specific harp, meteorological formulae smoothing 2d fields included. package automatically attached harp packages.","code":""},{"path":"/index.html","id":"installation","dir":"","previous_headings":"","what":"Installation","title":"Core functions and methods for the harp ecosystem","text":"can install harpCore GitHub :","code":"# install.packages(\"remotes\") remotes::install_github(\"harphub/harpCore\")"},{"path":"/reference/arrange.harp_list.html","id":null,"dir":"Reference","previous_headings":"","what":"Arrange rows by column values — arrange.harp_list","title":"Arrange rows by column values — arrange.harp_list","text":"harp_list method arrange(). works exactly way except harp_list returned. use function, dplyr package must attached .harp_list suffix can dropped.","code":""},{"path":"/reference/arrange.harp_list.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Arrange rows by column values — arrange.harp_list","text":"","code":"# S3 method for harp_list arrange(.data, ..., .by_group = FALSE)"},{"path":"/reference/arrange.harp_list.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Arrange rows by column values — arrange.harp_list","text":".data data frame, data frame extension (e.g. tibble), lazy data frame (e.g. dbplyr dtplyr). See Methods, , details. ... <data-masking> Variables, functions variables. Use desc() sort variable descending order. .by_group TRUE, sort first grouping variable. Applies grouped data frames .","code":""},{"path":"/reference/as_det.html","id":null,"dir":"Reference","previous_headings":"","what":"Convert an ensemble forecast to deterministic. — as_det","title":"Convert an ensemble forecast to deterministic. — as_det","text":"Convert ensemble forecast deterministic.","code":""},{"path":"/reference/as_det.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Convert an ensemble forecast to deterministic. — as_det","text":"","code":"as_det(x, member = NULL, sub_model = NULL)"},{"path":"/reference/as_det.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Convert an ensemble forecast to deterministic. — as_det","text":"x harp data frame harp list member need supplied one ensemble member x. Otherwise can member number, full name member column treat deterministic. sub_model multimodel ensembles may columns member number. Set name sub-model select member sub-model.","code":""},{"path":"/reference/as_det.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Convert an ensemble forecast to deterministic. — as_det","text":"harp deterministic data frame.","code":""},{"path":"/reference/as_det.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Convert an ensemble forecast to deterministic. — as_det","text":"","code":"as_det(ens_point_df, 0) #> ::deterministic point forecast:: # A tibble: 48 × 6 #>    fcst_model fcst_dttm           lead_time valid_dttm            SID   fcst #>  * <chr>      <dttm>                  <dbl> <dttm>              <dbl>  <dbl> #>  1 point      2021-01-01 00:00:00         0 2021-01-01 00:00:00  1001 0.277  #>  2 point      2021-01-01 00:00:00         1 2021-01-01 01:00:00  1001 0.650  #>  3 point      2021-01-01 00:00:00         2 2021-01-01 02:00:00  1001 0.601  #>  4 point      2021-01-01 00:00:00         3 2021-01-01 03:00:00  1001 0.427  #>  5 point      2021-01-01 00:00:00         4 2021-01-01 04:00:00  1001 0.0798 #>  6 point      2021-01-01 00:00:00         5 2021-01-01 05:00:00  1001 0.762  #>  7 point      2021-01-01 00:00:00         6 2021-01-01 06:00:00  1001 0.347  #>  8 point      2021-01-01 00:00:00         7 2021-01-01 07:00:00  1001 0.488  #>  9 point      2021-01-01 00:00:00         8 2021-01-01 08:00:00  1001 0.438  #> 10 point      2021-01-01 00:00:00         9 2021-01-01 09:00:00  1001 0.662  #> # ℹ 38 more rows as_det(ens_grid_df, 1) #> ::deterministic gridded forecast:: # A tibble: 24 × 5 #>    fcst_model fcst_dttm           lead_time valid_dttm               fcst #>  * <chr>      <dttm>                  <dbl> <dttm>              <geolist> #>  1 grid       2021-01-01 00:00:00         0 2021-01-01 00:00:00   [5 × 5] #>  2 grid       2021-01-01 00:00:00         1 2021-01-01 01:00:00   [5 × 5] #>  3 grid       2021-01-01 00:00:00         2 2021-01-01 02:00:00   [5 × 5] #>  4 grid       2021-01-01 00:00:00         3 2021-01-01 03:00:00   [5 × 5] #>  5 grid       2021-01-01 00:00:00         4 2021-01-01 04:00:00   [5 × 5] #>  6 grid       2021-01-01 00:00:00         5 2021-01-01 05:00:00   [5 × 5] #>  7 grid       2021-01-01 00:00:00         6 2021-01-01 06:00:00   [5 × 5] #>  8 grid       2021-01-01 00:00:00         7 2021-01-01 07:00:00   [5 × 5] #>  9 grid       2021-01-01 00:00:00         8 2021-01-01 08:00:00   [5 × 5] #> 10 grid       2021-01-01 00:00:00         9 2021-01-01 09:00:00   [5 × 5] #> # ℹ 14 more rows as_det(ens_point_list, 0) #> • a #> ::deterministic point forecast:: # A tibble: 48 × 6 #>    fcst_model fcst_dttm           lead_time valid_dttm            SID   fcst #>  * <chr>      <dttm>                  <dbl> <dttm>              <dbl>  <dbl> #>  1 a          2021-01-01 00:00:00         0 2021-01-01 00:00:00  1001 0.461  #>  2 a          2021-01-01 00:00:00         1 2021-01-01 01:00:00  1001 0.674  #>  3 a          2021-01-01 00:00:00         2 2021-01-01 02:00:00  1001 0.944  #>  4 a          2021-01-01 00:00:00         3 2021-01-01 03:00:00  1001 0.169  #>  5 a          2021-01-01 00:00:00         4 2021-01-01 04:00:00  1001 0.729  #>  6 a          2021-01-01 00:00:00         5 2021-01-01 05:00:00  1001 0.191  #>  7 a          2021-01-01 00:00:00         6 2021-01-01 06:00:00  1001 0.266  #>  8 a          2021-01-01 00:00:00         7 2021-01-01 07:00:00  1001 0.0762 #>  9 a          2021-01-01 00:00:00         8 2021-01-01 08:00:00  1001 0.308  #> 10 a          2021-01-01 00:00:00         9 2021-01-01 09:00:00  1001 0.630  #> # ℹ 38 more rows #>  #> • b #> ::deterministic point forecast:: # A tibble: 48 × 6 #>    fcst_model fcst_dttm           lead_time valid_dttm            SID  fcst #>  * <chr>      <dttm>                  <dbl> <dttm>              <dbl> <dbl> #>  1 b          2021-01-01 00:00:00         0 2021-01-01 00:00:00  1001 0.887 #>  2 b          2021-01-01 00:00:00         1 2021-01-01 01:00:00  1001 0.401 #>  3 b          2021-01-01 00:00:00         2 2021-01-01 02:00:00  1001 0.534 #>  4 b          2021-01-01 00:00:00         3 2021-01-01 03:00:00  1001 0.509 #>  5 b          2021-01-01 00:00:00         4 2021-01-01 04:00:00  1001 0.442 #>  6 b          2021-01-01 00:00:00         5 2021-01-01 05:00:00  1001 0.319 #>  7 b          2021-01-01 00:00:00         6 2021-01-01 06:00:00  1001 0.971 #>  8 b          2021-01-01 00:00:00         7 2021-01-01 07:00:00  1001 0.452 #>  9 b          2021-01-01 00:00:00         8 2021-01-01 08:00:00  1001 0.547 #> 10 b          2021-01-01 00:00:00         9 2021-01-01 09:00:00  1001 0.339 #> # ℹ 38 more rows #>   # Name the member column explicitly as_det(ens_point_df, \"point_mbr000\") #> ::deterministic point forecast:: # A tibble: 48 × 6 #>    fcst_model fcst_dttm           lead_time valid_dttm            SID   fcst #>  * <chr>      <dttm>                  <dbl> <dttm>              <dbl>  <dbl> #>  1 point      2021-01-01 00:00:00         0 2021-01-01 00:00:00  1001 0.277  #>  2 point      2021-01-01 00:00:00         1 2021-01-01 01:00:00  1001 0.650  #>  3 point      2021-01-01 00:00:00         2 2021-01-01 02:00:00  1001 0.601  #>  4 point      2021-01-01 00:00:00         3 2021-01-01 03:00:00  1001 0.427  #>  5 point      2021-01-01 00:00:00         4 2021-01-01 04:00:00  1001 0.0798 #>  6 point      2021-01-01 00:00:00         5 2021-01-01 05:00:00  1001 0.762  #>  7 point      2021-01-01 00:00:00         6 2021-01-01 06:00:00  1001 0.347  #>  8 point      2021-01-01 00:00:00         7 2021-01-01 07:00:00  1001 0.488  #>  9 point      2021-01-01 00:00:00         8 2021-01-01 08:00:00  1001 0.438  #> 10 point      2021-01-01 00:00:00         9 2021-01-01 09:00:00  1001 0.662  #> # ℹ 38 more rows"},{"path":"/reference/as_dttm.html","id":null,"dir":"Reference","previous_headings":"","what":"Convert to date-time (POSIXct) object — as_dttm","title":"Convert to date-time (POSIXct) object — as_dttm","text":"Given string numbers, conversion done POSIXct date-time class. string assumed form YYYYMMDD, YYYYMMDDhh, YYYYMMDDhhmm, YYYYMMDDhhmmss time zone UTC.","code":""},{"path":"/reference/as_dttm.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Convert to date-time (POSIXct) object — as_dttm","text":"","code":"as_dttm(x)"},{"path":"/reference/as_dttm.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Convert to date-time (POSIXct) object — as_dttm","text":"x numeric character string numeric object","code":""},{"path":"/reference/as_dttm.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Convert to date-time (POSIXct) object — as_dttm","text":"date-time object class POSIXct","code":""},{"path":"/reference/as_dttm.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Convert to date-time (POSIXct) object — as_dttm","text":"","code":"# Character input as_dttm(\"20220203\") #> [1] \"2022-02-03 UTC\" as_dttm(\"2022020306\") #> [1] \"2022-02-03 06:00:00 UTC\" as_dttm(\"202202030630\") #> [1] \"2022-02-03 06:30:00 UTC\" as_dttm(\"20220203063022\") #> [1] \"2022-02-03 06:30:22 UTC\" # # Numeric input as_dttm(20220203) #> [1] \"2022-02-03 UTC\" as_dttm(2022020306) #> [1] \"2022-02-03 06:00:00 UTC\" as_dttm(202202030630) #> [1] \"2022-02-03 06:30:00 UTC\" as_dttm(20220203063022) #> [1] \"2022-02-03 06:30:22 UTC\""},{"path":"/reference/as_harp_df.html","id":null,"dir":"Reference","previous_headings":"","what":"Coerce to a harp_df data frame — as_harp_df","title":"Coerce to a harp_df data frame — as_harp_df","text":"harp uses classes dispatch data correct methods running function. as_harp_df takes data frame scans column names types assign correct classes. input data frame tibble coerced one ensure easier read print method.","code":""},{"path":"/reference/as_harp_df.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Coerce to a harp_df data frame — as_harp_df","text":"","code":"as_harp_df(x)  is_harp_df(x)"},{"path":"/reference/as_harp_df.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Coerce to a harp_df data frame — as_harp_df","text":"x data frame valid_dttm column.","code":""},{"path":"/reference/as_harp_df.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Coerce to a harp_df data frame — as_harp_df","text":"data frame appropriate class","code":""},{"path":"/reference/as_harp_df.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Coerce to a harp_df data frame — as_harp_df","text":"","code":"library(dplyr) #>  #> Attaching package: ‘dplyr’ #> The following objects are masked from ‘package:stats’: #>  #>     filter, lag #> The following objects are masked from ‘package:base’: #>  #>     intersect, setdiff, setequal, union d_f <- data.frame(valid_dttm = as_dttm(seq_dttm(2021010100, 2021010123))) as_harp_df(d_f) #> # A tibble: 24 × 1 #>    valid_dttm          #>  * <dttm>              #>  1 2021-01-01 00:00:00 #>  2 2021-01-01 01:00:00 #>  3 2021-01-01 02:00:00 #>  4 2021-01-01 03:00:00 #>  5 2021-01-01 04:00:00 #>  6 2021-01-01 05:00:00 #>  7 2021-01-01 06:00:00 #>  8 2021-01-01 07:00:00 #>  9 2021-01-01 08:00:00 #> 10 2021-01-01 09:00:00 #> # ℹ 14 more rows as_harp_df(mutate(d_f, fcst_det = runif(24))) #> ::deterministic point forecast:: # A tibble: 24 × 3 #>    fcst_model valid_dttm           fcst #>  * <chr>      <dttm>              <dbl> #>  1 fcst       2021-01-01 00:00:00 0.631 #>  2 fcst       2021-01-01 01:00:00 0.471 #>  3 fcst       2021-01-01 02:00:00 0.684 #>  4 fcst       2021-01-01 03:00:00 0.324 #>  5 fcst       2021-01-01 04:00:00 0.991 #>  6 fcst       2021-01-01 05:00:00 0.168 #>  7 fcst       2021-01-01 06:00:00 0.455 #>  8 fcst       2021-01-01 07:00:00 0.454 #>  9 fcst       2021-01-01 08:00:00 0.723 #> 10 fcst       2021-01-01 09:00:00 0.905 #> # ℹ 14 more rows as_harp_df(mutate(d_f, fcst_mbr000 = runif(24), fcst_mbr001 = runif(24))) #> ::ensemble point forecast:: # A tibble: 24 × 3 #>    valid_dttm          fcst_mbr000 fcst_mbr001 #>  * <dttm>                    <dbl>       <dbl> #>  1 2021-01-01 00:00:00       0.316      0.0522 #>  2 2021-01-01 01:00:00       0.839      0.161  #>  3 2021-01-01 02:00:00       0.840      0.757  #>  4 2021-01-01 03:00:00       0.441      0.840  #>  5 2021-01-01 04:00:00       0.848      0.665  #>  6 2021-01-01 05:00:00       0.313      0.383  #>  7 2021-01-01 06:00:00       0.228      0.451  #>  8 2021-01-01 07:00:00       0.818      0.370  #>  9 2021-01-01 08:00:00       0.496      0.576  #> 10 2021-01-01 09:00:00       0.645      0.666  #> # ℹ 14 more rows # Note the class class(as_harp_df(d_f)) #> [1] \"harp_point_df\" \"harp_df\"       \"tbl_df\"        \"tbl\"           #> [5] \"data.frame\"    class(as_harp_df(mutate(d_f, fcst_det = runif(24)))) #> [1] \"harp_det_point_df\" \"harp_point_df\"     \"harp_df\"           #> [4] \"tbl_df\"            \"tbl\"               \"data.frame\"        class(as_harp_df(mutate(d_f, fcst_mbr000 = runif(24), fcst_mbr001 = runif(24)))) #> [1] \"harp_ens_point_df\" \"harp_point_df\"     \"harp_df\"           #> [4] \"tbl_df\"            \"tbl\"               \"data.frame\""},{"path":"/reference/as_harp_list.html","id":null,"dir":"Reference","previous_headings":"","what":"Create a list of harp data frames — as_harp_list","title":"Create a list of harp data frames — as_harp_list","text":"Create list harp data frames","code":""},{"path":"/reference/as_harp_list.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Create a list of harp data frames — as_harp_list","text":"","code":"as_harp_list(...)  is_harp_list(x)"},{"path":"/reference/as_harp_list.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Create a list of harp data frames — as_harp_list","text":"... harp_df data frames. Must named x object test.","code":""},{"path":"/reference/as_harp_list.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Create a list of harp data frames — as_harp_list","text":"harp_list list","code":""},{"path":"/reference/as_harp_list.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Create a list of harp data frames — as_harp_list","text":"","code":"as_harp_list(   a = as_harp_df(data.frame(     valid_dttm = as_dttm(seq_dttm(2021010100, 2021010123)),     a_det = runif(24)   )),   b = as_harp_df(data.frame(     valid_dttm = as_dttm(seq_dttm(2021010100, 2021010123)),     b_det = runif(24)   )) ) #> • a #> ::deterministic point forecast:: # A tibble: 24 × 3 #>    fcst_model valid_dttm           fcst #>  * <chr>      <dttm>              <dbl> #>  1 a          2021-01-01 00:00:00 0.917 #>  2 a          2021-01-01 01:00:00 0.205 #>  3 a          2021-01-01 02:00:00 0.630 #>  4 a          2021-01-01 03:00:00 0.457 #>  5 a          2021-01-01 04:00:00 0.951 #>  6 a          2021-01-01 05:00:00 0.318 #>  7 a          2021-01-01 06:00:00 0.674 #>  8 a          2021-01-01 07:00:00 0.813 #>  9 a          2021-01-01 08:00:00 0.867 #> 10 a          2021-01-01 09:00:00 0.167 #> # ℹ 14 more rows #>  #> • b #> ::deterministic point forecast:: # A tibble: 24 × 3 #>    fcst_model valid_dttm            fcst #>  * <chr>      <dttm>               <dbl> #>  1 b          2021-01-01 00:00:00 0.616  #>  2 b          2021-01-01 01:00:00 0.515  #>  3 b          2021-01-01 02:00:00 0.383  #>  4 b          2021-01-01 03:00:00 0.0416 #>  5 b          2021-01-01 04:00:00 0.427  #>  6 b          2021-01-01 05:00:00 0.641  #>  7 b          2021-01-01 06:00:00 0.627  #>  8 b          2021-01-01 07:00:00 0.0172 #>  9 b          2021-01-01 08:00:00 0.665  #> 10 b          2021-01-01 09:00:00 0.924  #> # ℹ 14 more rows #>  as_harp_list(   a = as_harp_df(data.frame(     valid_dttm = as_dttm(seq_dttm(2021010100, 2021010123)),     a_mbr000  = runif(24),     a_mbr001  = runif(24)   )),   b = as_harp_df(data.frame(     valid_dttm = as_dttm(seq_dttm(2021010100, 2021010123)),     b_mbr000 = runif(24),     b_mbr001 = runif(24)   )) ) #> • a #> ::ensemble point forecast:: # A tibble: 24 × 3 #>    valid_dttm          a_mbr000 a_mbr001 #>  * <dttm>                 <dbl>    <dbl> #>  1 2021-01-01 00:00:00  0.713     0.0485 #>  2 2021-01-01 01:00:00  0.766     0.129  #>  3 2021-01-01 02:00:00  0.155     0.670  #>  4 2021-01-01 03:00:00  0.938     0.0418 #>  5 2021-01-01 04:00:00  0.00572   0.319  #>  6 2021-01-01 05:00:00  0.236     0.372  #>  7 2021-01-01 06:00:00  0.0867    0.473  #>  8 2021-01-01 07:00:00  0.448     0.275  #>  9 2021-01-01 08:00:00  0.322     0.715  #> 10 2021-01-01 09:00:00  0.459     0.248  #> # ℹ 14 more rows #>  #> • b #> ::ensemble point forecast:: # A tibble: 24 × 3 #>    valid_dttm          b_mbr000 b_mbr001 #>  * <dttm>                 <dbl>    <dbl> #>  1 2021-01-01 00:00:00   0.138     0.400 #>  2 2021-01-01 01:00:00   0.865     0.409 #>  3 2021-01-01 02:00:00   0.0623    0.260 #>  4 2021-01-01 03:00:00   0.501     0.989 #>  5 2021-01-01 04:00:00   0.964     0.378 #>  6 2021-01-01 05:00:00   0.727     0.247 #>  7 2021-01-01 06:00:00   0.547     0.223 #>  8 2021-01-01 07:00:00   0.551     0.515 #>  9 2021-01-01 08:00:00   0.558     0.160 #> 10 2021-01-01 09:00:00   0.321     0.280 #> # ℹ 14 more rows #>"},{"path":"/reference/as_str_dttm.html","id":null,"dir":"Reference","previous_headings":"","what":"Convert to date-time string — as_str_dttm","title":"Convert to date-time string — as_str_dttm","text":"functions take POSIXct numeric object convert date-time string format specified function. Years always 4 digits long parts string 2 digits long leading zeros necessary. case numeric input, number assumed seconds since 1970-01-01 00:00:00 UTC.","code":""},{"path":"/reference/as_str_dttm.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Convert to date-time string — as_str_dttm","text":"","code":"as_str_dttm(x)  as_YMD(x)  as_ymd(x)  as_YMDh(x)  as_ymdh(x)  as_YMDhm(x)  as_ymdhm(x)  as_YMDhms(x)  as_ymdhms(x)"},{"path":"/reference/as_str_dttm.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Convert to date-time string — as_str_dttm","text":"x POSIXct numeric object seconds since 1970-01-01 00:00:00","code":""},{"path":"/reference/as_str_dttm.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Convert to date-time string — as_str_dttm","text":"date-time string","code":""},{"path":"/reference/as_str_dttm.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Convert to date-time string — as_str_dttm","text":"as_str_dttm returns string trailing zeros truncated. functions named output truncated precision given function name. Note rounding done, truncation. lower case versions functions well Y, M, D upper case.","code":""},{"path":"/reference/as_str_dttm.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Convert to date-time string — as_str_dttm","text":"","code":"as_str_dttm(as.POSIXct(\"2022-03-08 00:00:00\")) #> [1] \"20220308\" as_str_dttm(as.POSIXct(\"2022-03-08 10:00:00\")) #> [1] \"2022030810\" as_str_dttm(as.POSIXct(\"2022-03-08 10:30:00\")) #> [1] \"202203081030\" as_str_dttm(as.POSIXct(\"2022-03-08 10:30:20\")) #> [1] \"20220308103020\" as_YMD(Sys.time()) #> [1] \"20231116\" as_YMDh(Sys.time()) #> [1] \"2023111611\" as_YMDhm(Sys.time()) #> [1] \"202311161117\" as_YMDhms(Sys.time()) #> [1] \"20231116111750\" as_YMD(as.numeric(Sys.time())) #> [1] \"20231116\" as_YMDh(as.numeric(Sys.time())) #> [1] \"2023111610\" as_YMDhm(as.numeric(Sys.time())) #> [1] \"202311161017\" as_YMDhms(as.numeric(Sys.time())) #> [1] \"20231116101750\""},{"path":"/reference/as_unixtime.html","id":null,"dir":"Reference","previous_headings":"","what":"Convert to UNIX time - seconds since 1970-01-01 00:00:00 — as_unixtime","title":"Convert to UNIX time - seconds since 1970-01-01 00:00:00 — as_unixtime","text":"Given string numbers POSIXct date-time object, conversion done UNIX time, .e. seconds since 1970-01-01 00:00:00. string assumed form YYYYMMDD, YYYYMMDDhh, YYYYMMDDhhmm, YYYYMMDDhhmmss time zone UTC.","code":""},{"path":"/reference/as_unixtime.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Convert to UNIX time - seconds since 1970-01-01 00:00:00 — as_unixtime","text":"","code":"as_unixtime(x)"},{"path":"/reference/as_unixtime.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Convert to UNIX time - seconds since 1970-01-01 00:00:00 — as_unixtime","text":"x numeric character string, numeric POSIXct object","code":""},{"path":"/reference/as_unixtime.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Convert to UNIX time - seconds since 1970-01-01 00:00:00 — as_unixtime","text":"Numeric - seconds since 1970-01-01 00:00:00","code":""},{"path":"/reference/as_unixtime.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Convert to UNIX time - seconds since 1970-01-01 00:00:00 — as_unixtime","text":"","code":"# Character input as_unixtime(\"20220203\") #> [1] 1643846400 as_unixtime(\"2022020306\") #> [1] 1643868000 as_unixtime(\"202202030630\") #> [1] 1643869800 as_unixtime(\"20220203063022\") #> [1] 1643869822 # # Numeric input as_unixtime(20220203) #> [1] 1643846400 as_unixtime(2022020306) #> [1] 1643868000 as_unixtime(202202030630) #> [1] 1643869800 as_unixtime(20220203063022) #> [1] 1643869822 # # POSIXct input as_unixtime(Sys.time()) #> [1] 1700129870"},{"path":"/reference/bind.html","id":null,"dir":"Reference","previous_headings":"","what":"Bind data frames in a list — bind","title":"Bind data frames in a list — bind","text":"bind wrapper around bind_rows dedicated method harp_list objects. cases bind_rows called.","code":""},{"path":"/reference/bind.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Bind data frames in a list — bind","text":"","code":"bind(..., .id = NULL)"},{"path":"/reference/bind.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Bind data frames in a list — bind","text":"... list harp_list object .id name column used identify element list resulting data frame","code":""},{"path":"/reference/bind.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Bind data frames in a list — bind","text":"data frame class first data frame list","code":""},{"path":"/reference/bind.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Bind data frames in a list — bind","text":"harp_list objects, name element list added data frame column, heading given .id argument. forecast data prefix \"_det\" deteriministic forcasts prefix \"_mbr***\" ensemble forecasts removed column headings. multimodel ensembles, lead problems columns heading. noted check made classes data frames harp_list object, class first used output.","code":""},{"path":"/reference/bind.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Bind data frames in a list — bind","text":"","code":"bind(   as_harp_list(     a = as_harp_df(data.frame(       valid_dttm = as_dttm(seq_dttm(2021010100, 2021010123)),       a_det = runif(24)     )),     b = as_harp_df(data.frame(       valid_dttm = as_dttm(seq_dttm(2021010100, 2021010123)),       b_det = runif(24)     ))   ) ) #> ::deterministic point forecast:: # A tibble: 48 × 3 #>    fcst_model valid_dttm            fcst #>    <chr>      <dttm>               <dbl> #>  1 a          2021-01-01 00:00:00 0.305  #>  2 a          2021-01-01 01:00:00 0.798  #>  3 a          2021-01-01 02:00:00 0.0561 #>  4 a          2021-01-01 03:00:00 0.978  #>  5 a          2021-01-01 04:00:00 0.331  #>  6 a          2021-01-01 05:00:00 0.0619 #>  7 a          2021-01-01 06:00:00 0.944  #>  8 a          2021-01-01 07:00:00 0.734  #>  9 a          2021-01-01 08:00:00 0.629  #> 10 a          2021-01-01 09:00:00 0.179  #> # ℹ 38 more rows bind(   as_harp_list(     a = as_harp_df(data.frame(       valid_dttm = as_dttm(seq_dttm(2021010100, 2021010123)),       a_mbr000  = runif(24),       a_mbr001  = runif(24)     )),     b = as_harp_df(data.frame(       valid_dttm = as_dttm(seq_dttm(2021010100, 2021010123)),       b_mbr000 = runif(24),       b_mbr001 = runif(24)     ))   ) ) #> ::ensemble point forecast [[long]]:: # A tibble: 96 × 5 #>    fcst_model sub_model valid_dttm          member   fcst #>    <chr>      <chr>     <dttm>              <chr>   <dbl> #>  1 a          a         2021-01-01 00:00:00 mbr000 0.877  #>  2 a          a         2021-01-01 00:00:00 mbr001 0.0969 #>  3 a          a         2021-01-01 01:00:00 mbr000 0.479  #>  4 a          a         2021-01-01 01:00:00 mbr001 0.910  #>  5 a          a         2021-01-01 02:00:00 mbr000 0.777  #>  6 a          a         2021-01-01 02:00:00 mbr001 0.557  #>  7 a          a         2021-01-01 03:00:00 mbr000 0.292  #>  8 a          a         2021-01-01 03:00:00 mbr001 0.700  #>  9 a          a         2021-01-01 04:00:00 mbr000 0.498  #> 10 a          a         2021-01-01 04:00:00 mbr001 0.684  #> # ℹ 86 more rows"},{"path":"/reference/common_cases.html","id":null,"dir":"Reference","previous_headings":"","what":"Filter to common cases — common_cases","title":"Filter to common cases — common_cases","text":"fair comparison models, verification done dates locations common models. common_cases takes harp_list object input identifies filters cases common forecast models harp_list object. default done SID, fcst_dttm lead_time columns, extra columns can added via .... one columns vertical coordinate (\"p\", \"z\", \"ml\" pressure, height model level respectively), column also included.","code":""},{"path":"/reference/common_cases.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Filter to common cases — common_cases","text":"","code":"common_cases(.fcst, ...)"},{"path":"/reference/common_cases.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Filter to common cases — common_cases","text":".fcst harp_list object ... Extra columns determine common cases. remove one default columns test use -","code":""},{"path":"/reference/common_cases.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Filter to common cases — common_cases","text":"input data frame common stations forecast dates forecast model selected.","code":""},{"path":"/reference/decum.html","id":null,"dir":"Reference","previous_headings":"","what":"Decumulate accumulated variables — decum","title":"Decumulate accumulated variables — decum","text":"Many models store accumulated values amount accumulated since start forecast. decum() decumulates values specific time periods. Note time periods overlapping, hourly data (example) 6-hour accumulation calculated every hour previous 6 hours rather consecutive 6-hour windows.","code":""},{"path":"/reference/decum.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Decumulate accumulated variables — decum","text":"","code":"decum(.data, decum_time, cols, time_col, group_col, df_name = NULL, ...)"},{"path":"/reference/decum.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Decumulate accumulated variables — decum","text":".data harp_df data fram harp_list decum_time time period decumulate data. numeric, considered hours, otherwise string number followed unit. Units can \"s\", seconds; \"m\", minutes; \"h\", hours; \"d\", days. cols Columns decumulation. Uses semantics select. time_col time column use calculating decumulation windows. Uses semantics select. group_col columns group output . Uses semantics select. df_name name data frame - really used generate error messages. ... Used methods.","code":""},{"path":"/reference/decum.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Decumulate accumulated variables — decum","text":"object class .data containing decumulated values.","code":""},{"path":"/reference/define_domain.html","id":null,"dir":"Reference","previous_headings":"","what":"Define a geodomain — define_domain","title":"Define a geodomain — define_domain","text":"function used define new domain regular grid. minimum, projection, geographic location centre domain number horizontal resolution grid points must provided.","code":""},{"path":"/reference/define_domain.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Define a geodomain — define_domain","text":"","code":"define_domain(   centre_lon,   centre_lat,   nxny,   dxdy,   proj = c(\"lambert\", \"lcc\", \"merc\", \"mercator\", \"omerc\", \"tmerc\", \"somerc\", \"lalo\",     \"longlat\", \"latlong\", \"ob_tran\", \"rot_longlat\", \"rot_latlong\", \"RotLatLon\", \"stere\",     \"stereo\", \"stereographic\"),   ref_lon,   ref_lat,   exey = NULL,   tilt = 0,   R = 6371229,   ... )"},{"path":"/reference/define_domain.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Define a geodomain — define_domain","text":"centre_lon longitude centre domain decimal degrees. centre_lat latitude centre domain decimal degrees. nxny number grid points x y directions. vector length 2 number grid points x direction first. 1 value given assumed directions. dxdy horizontal resolution grid x y directions. lat-lon projections decimal degrees, otherwise metres. vector length 2 resolution x direction first. proj projection domain. See details. ref_lon reference longitude projection needed. ref_lat reference latitude projection needed. exey defining grid extension zone, vector length 2 number grid points x y directions extension zone. 1 value given assumed directions. tilt tilt used rotated Mercator projection. R radius earth metres. default 6371229m. ... arguments describing shape earth proj format.","code":""},{"path":"/reference/define_domain.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Define a geodomain — define_domain","text":"geodomain","code":""},{"path":"/reference/define_domain.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Define a geodomain — define_domain","text":"","code":"dd <- define_domain(10, 60, 1000, 2500) # Default lambert projection plot(dd)   dd <- define_domain(0, 0, c(360, 180), 1, \"latlong\") # Whole earth plot(dd)"},{"path":"/reference/deharp.html","id":null,"dir":"Reference","previous_headings":"","what":"Remove harp classes — deharp","title":"Remove harp classes — deharp","text":"cases may need remove harp classes data frame, example methods exist. Use function remove harp related classes object","code":""},{"path":"/reference/deharp.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Remove harp classes — deharp","text":"","code":"deharp(x)"},{"path":"/reference/deharp.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Remove harp classes — deharp","text":"x object.","code":""},{"path":"/reference/deharp.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Remove harp classes — deharp","text":"x harp classes removed.","code":""},{"path":"/reference/deharp.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Remove harp classes — deharp","text":"","code":"class(det_point_df) #> [1] \"harp_det_point_df\" \"harp_point_df\"     \"harp_df\"           #> [4] \"tbl_df\"            \"tbl\"               \"data.frame\"        class(deharp(det_point_df)) #> [1] \"tbl_df\"     \"tbl\"        \"data.frame\""},{"path":"/reference/ens_mean_and_var.html","id":null,"dir":"Reference","previous_headings":"","what":"Compute the ensemble mean and variance — ens_mean_and_var","title":"Compute the ensemble mean and variance — ens_mean_and_var","text":"function superseded ens_stats. However, ens_mean_and_var() still useful computing ensemble spread dropped member used ens_spread_and_skill.","code":""},{"path":"/reference/ens_mean_and_var.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Compute the ensemble mean and variance — ens_mean_and_var","text":"","code":"ens_mean_and_var(   .fcst,   mean_name = \"ens_mean\",   var_name = \"ens_var\",   sd_name = \"ens_spread\",   var_drop_member = NULL )"},{"path":"/reference/ens_mean_and_var.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Compute the ensemble mean and variance — ens_mean_and_var","text":".fcst harp_list object, harp_ens_point_df harp_ens_grid_df data frame. mean_name output column name ensemble mean var_name output column name ensemble variance sd_name output column name ensemble spread (standard deviation) var_drop_member members drop calculation ensemble variance standard deviation. harp_fcst objects, can numeric scalar - case recycled forecast models; list numeric vector length harp_fcst object, named list names corresponding names harp_fcst object. ...","code":""},{"path":"/reference/ens_mean_and_var.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Compute the ensemble mean and variance — ens_mean_and_var","text":"harp_fcst object columns ens_mean ens_var added forecast tables.","code":""},{"path":"/reference/ens_mean_and_var.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Compute the ensemble mean and variance — ens_mean_and_var","text":"ensemble mean variance computed added columns tables harp_df object harp_list object..","code":""},{"path":"/reference/ens_prob.html","id":null,"dir":"Reference","previous_headings":"","what":"Compute the ensemble probability for a threshold — ens_prob","title":"Compute the ensemble probability for a threshold — ens_prob","text":"probability threshold ensemble computed. default threshold exceedence (P(fcst >= threshold)), probability threshold, outside two thresholds can also calculated.","code":""},{"path":"/reference/ens_prob.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Compute the ensemble probability for a threshold — ens_prob","text":"","code":"ens_prob(   x,   threshold = 0,   comparator = c(\"ge\", \"gt\", \"le\", \"lt\", \"between\", \"outside\"),   include_low = TRUE,   include_high = TRUE,   ... )"},{"path":"/reference/ens_prob.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Compute the ensemble probability for a threshold — ens_prob","text":"x harp_ens_grid_df harp_ens_point_df data frame, geolist, harp_list containing ensemble data frames. threshold threshold computing binary probabilities. comparator = \"\", must two element vector. Set NA use raw data. comparator compare x threshold compute binary probabilities. Can \"ge\", \"gt\", \"le\", \"lt\" >=, >, <= < respectively. Can also \"\" \"outside\", case binary probability outside two values given threshold computed. include_low Logical. Whether include lower two thresholds comparison comparator = \"\" comparator = \"outside\". include_high Logical. Whether include higher two thresholds comparison comparator = \"\" comparator = \"outside\". ... Used methods.","code":""},{"path":"/reference/ens_prob.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Compute the ensemble probability for a threshold — ens_prob","text":"object class x probabilities computed","code":""},{"path":"/reference/ens_prob.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Compute the ensemble probability for a threshold — ens_prob","text":"Note geolist passed function, element geolist assumed ensemble member.","code":""},{"path":"/reference/ens_prob.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Compute the ensemble probability for a threshold — ens_prob","text":"","code":"p_ge_0.5 <- ens_prob(ens_grid_df, 0.5) image(p_ge_0.5$prob_ge_0.5[[1]])   p_le_0.1 <- ens_prob(ens_grid_df, 0.1, comparator = \"le\") image(p_le_0.1$prob_le_0.1[[1]])   p_btw_0.25_0.75 <- ens_prob(   ens_grid_df, c(0.25, 0.75), comparator = \"between\" ) image(p_btw_0.25_0.75$prob_between_0.25_0.75[[1]])"},{"path":"/reference/ens_stats.html","id":null,"dir":"Reference","previous_headings":"","what":"Compute basic ensemble statistics — ens_stats","title":"Compute basic ensemble statistics — ens_stats","text":"Given data frame ensemble forecasts ensemble mean, ensemble standard deviation (spread), ensemble variance, ensemble minimum, ensemble calculated.","code":""},{"path":"/reference/ens_stats.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Compute basic ensemble statistics — ens_stats","text":"","code":"ens_stats(   .fcst,   mean = TRUE,   sd = TRUE,   var = FALSE,   min = FALSE,   max = FALSE,   keep_members = FALSE,   ... )  # S3 method for harp_ens_point_df ens_stats(   .fcst,   mean = TRUE,   sd = TRUE,   var = FALSE,   min = FALSE,   max = FALSE,   keep_members = FALSE,   median = FALSE,   ... )"},{"path":"/reference/ens_stats.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Compute basic ensemble statistics — ens_stats","text":".fcst harp_ens_grid_df harp_ens_point_df data frame, harp_list containing data frames classes. mean Logical. Whether compute ensemble mean. sd Logical. Whether compute ensemble standard deviation. var Logical. Whether compute ensemble variance. min Logical. Whether compute ensemble minumum. max Logical. Whether compute ensemble maximum. keep_members Logical. Whether keep ensemble members returned object. default return statistics. ... used. median Logical. Whether compute ensemble median.","code":""},{"path":"/reference/ens_stats.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Compute basic ensemble statistics — ens_stats","text":"object class .fcst columns ensemble statistics","code":""},{"path":"/reference/ens_stats.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Compute basic ensemble statistics — ens_stats","text":"default ensemble mean standard deviation computed. Note ensemble median yet implemented gridded data.","code":""},{"path":"/reference/ens_stats.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Compute basic ensemble statistics — ens_stats","text":"","code":"ens_stats(ens_point_df) #> # A tibble: 48 × 6 #>    fcst_dttm           lead_time valid_dttm            SID ens_mean ens_sd #>  * <dttm>                  <dbl> <dttm>              <dbl>    <dbl>  <dbl> #>  1 2021-01-01 00:00:00         0 2021-01-01 00:00:00  1001    0.533 0.362  #>  2 2021-01-01 00:00:00         1 2021-01-01 01:00:00  1001    0.597 0.0752 #>  3 2021-01-01 00:00:00         2 2021-01-01 02:00:00  1001    0.528 0.104  #>  4 2021-01-01 00:00:00         3 2021-01-01 03:00:00  1001    0.552 0.177  #>  5 2021-01-01 00:00:00         4 2021-01-01 04:00:00  1001    0.306 0.320  #>  6 2021-01-01 00:00:00         5 2021-01-01 05:00:00  1001    0.434 0.463  #>  7 2021-01-01 00:00:00         6 2021-01-01 06:00:00  1001    0.377 0.0425 #>  8 2021-01-01 00:00:00         7 2021-01-01 07:00:00  1001    0.273 0.304  #>  9 2021-01-01 00:00:00         8 2021-01-01 08:00:00  1001    0.692 0.359  #> 10 2021-01-01 00:00:00         9 2021-01-01 09:00:00  1001    0.562 0.141  #> # ℹ 38 more rows ens_stats(ens_point_df, keep_members = TRUE) #> ::ensemble point forecast:: # A tibble: 48 × 8 #>    fcst_dttm           lead_time valid_dttm            SID point_mbr000 #>    <dttm>                  <dbl> <dttm>              <dbl>        <dbl> #>  1 2021-01-01 00:00:00         0 2021-01-01 00:00:00  1001       0.277  #>  2 2021-01-01 00:00:00         1 2021-01-01 01:00:00  1001       0.650  #>  3 2021-01-01 00:00:00         2 2021-01-01 02:00:00  1001       0.601  #>  4 2021-01-01 00:00:00         3 2021-01-01 03:00:00  1001       0.427  #>  5 2021-01-01 00:00:00         4 2021-01-01 04:00:00  1001       0.0798 #>  6 2021-01-01 00:00:00         5 2021-01-01 05:00:00  1001       0.762  #>  7 2021-01-01 00:00:00         6 2021-01-01 06:00:00  1001       0.347  #>  8 2021-01-01 00:00:00         7 2021-01-01 07:00:00  1001       0.488  #>  9 2021-01-01 00:00:00         8 2021-01-01 08:00:00  1001       0.438  #> 10 2021-01-01 00:00:00         9 2021-01-01 09:00:00  1001       0.662  #> # ℹ 38 more rows #> # ℹ 3 more variables: point_mbr001 <dbl>, ens_mean <dbl>, ens_sd <dbl> ens_stats(ens_point_df, var = TRUE, min = TRUE, max = TRUE) #> # A tibble: 48 × 9 #>    fcst_dttm           lead_time valid_dttm            SID ens_mean ens_sd #>  * <dttm>                  <dbl> <dttm>              <dbl>    <dbl>  <dbl> #>  1 2021-01-01 00:00:00         0 2021-01-01 00:00:00  1001    0.533 0.362  #>  2 2021-01-01 00:00:00         1 2021-01-01 01:00:00  1001    0.597 0.0752 #>  3 2021-01-01 00:00:00         2 2021-01-01 02:00:00  1001    0.528 0.104  #>  4 2021-01-01 00:00:00         3 2021-01-01 03:00:00  1001    0.552 0.177  #>  5 2021-01-01 00:00:00         4 2021-01-01 04:00:00  1001    0.306 0.320  #>  6 2021-01-01 00:00:00         5 2021-01-01 05:00:00  1001    0.434 0.463  #>  7 2021-01-01 00:00:00         6 2021-01-01 06:00:00  1001    0.377 0.0425 #>  8 2021-01-01 00:00:00         7 2021-01-01 07:00:00  1001    0.273 0.304  #>  9 2021-01-01 00:00:00         8 2021-01-01 08:00:00  1001    0.692 0.359  #> 10 2021-01-01 00:00:00         9 2021-01-01 09:00:00  1001    0.562 0.141  #> # ℹ 38 more rows #> # ℹ 3 more variables: ens_var <dbl>, ens_min <dbl>, ens_max <dbl> ens_stats(ens_grid_list) #> • a #> # A tibble: 24 × 6 #>    fcst_model fcst_dttm           lead_time valid_dttm          ens_mean  ens_sd #>  * <chr>      <dttm>                  <dbl> <dttm>              <geolis> <geoli> #>  1 a          2021-01-01 00:00:00         0 2021-01-01 00:00:00  [5 × 5] [5 × 5] #>  2 a          2021-01-01 00:00:00         1 2021-01-01 01:00:00  [5 × 5] [5 × 5] #>  3 a          2021-01-01 00:00:00         2 2021-01-01 02:00:00  [5 × 5] [5 × 5] #>  4 a          2021-01-01 00:00:00         3 2021-01-01 03:00:00  [5 × 5] [5 × 5] #>  5 a          2021-01-01 00:00:00         4 2021-01-01 04:00:00  [5 × 5] [5 × 5] #>  6 a          2021-01-01 00:00:00         5 2021-01-01 05:00:00  [5 × 5] [5 × 5] #>  7 a          2021-01-01 00:00:00         6 2021-01-01 06:00:00  [5 × 5] [5 × 5] #>  8 a          2021-01-01 00:00:00         7 2021-01-01 07:00:00  [5 × 5] [5 × 5] #>  9 a          2021-01-01 00:00:00         8 2021-01-01 08:00:00  [5 × 5] [5 × 5] #> 10 a          2021-01-01 00:00:00         9 2021-01-01 09:00:00  [5 × 5] [5 × 5] #> # ℹ 14 more rows #>  #> • b #> # A tibble: 24 × 6 #>    fcst_model fcst_dttm           lead_time valid_dttm          ens_mean  ens_sd #>  * <chr>      <dttm>                  <dbl> <dttm>              <geolis> <geoli> #>  1 b          2021-01-01 00:00:00         0 2021-01-01 00:00:00  [5 × 5] [5 × 5] #>  2 b          2021-01-01 00:00:00         1 2021-01-01 01:00:00  [5 × 5] [5 × 5] #>  3 b          2021-01-01 00:00:00         2 2021-01-01 02:00:00  [5 × 5] [5 × 5] #>  4 b          2021-01-01 00:00:00         3 2021-01-01 03:00:00  [5 × 5] [5 × 5] #>  5 b          2021-01-01 00:00:00         4 2021-01-01 04:00:00  [5 × 5] [5 × 5] #>  6 b          2021-01-01 00:00:00         5 2021-01-01 05:00:00  [5 × 5] [5 × 5] #>  7 b          2021-01-01 00:00:00         6 2021-01-01 06:00:00  [5 × 5] [5 × 5] #>  8 b          2021-01-01 00:00:00         7 2021-01-01 07:00:00  [5 × 5] [5 × 5] #>  9 b          2021-01-01 00:00:00         8 2021-01-01 08:00:00  [5 × 5] [5 × 5] #> 10 b          2021-01-01 00:00:00         9 2021-01-01 09:00:00  [5 × 5] [5 × 5] #> # ℹ 14 more rows #>"},{"path":"/reference/example_data.html","id":null,"dir":"Reference","previous_headings":"","what":"Data for examples — example_data","title":"Data for examples — example_data","text":"Data used demonstrating function usage examples. data random numbers sampled uniform distribution 0 1.","code":""},{"path":"/reference/example_data.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Data for examples — example_data","text":"","code":"det_point_df  ens_point_df  det_grid_df  ens_grid_df  det_point_list  ens_point_list  det_grid_list  ens_grid_list  anl_grid_df"},{"path":"/reference/example_data.html","id":"format","dir":"Reference","previous_headings":"","what":"Format","title":"Data for examples — example_data","text":"harp_df data frame harp_list fcst_dttm Forecast start date-time lead_time Forecast lead time hours valid_dttm Forecast valid date-time SID ID observation station point_det Deterministic point forecast value point_mbr000 Point forecast value ensemble member 000 point_mbr001 Point forecast value ensemble member 001 grid_det Deterministic gridded forecast grid_mbr000 Gridded forecast ensemble member 000 grid_mbr001 Gridded forecast ensemble member 001 grid_anl Gridded analysis","code":""},{"path":"/reference/expand_date.html","id":null,"dir":"Reference","previous_headings":"","what":"Expand date-time column in a data frame — expand_date","title":"Expand date-time column in a data frame — expand_date","text":"expand_date extracts year, month, day, hour minute date-time column data frame creates column part date. name date-time column ends \"date\", names new columns whatever precedes \"date\" original column name followed \"_year\", \"_month\", \"_day\", \"_hour\", \"_minute\". original column name end date suffixes pasted onto original column name.","code":""},{"path":"/reference/expand_date.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Expand date-time column in a data frame — expand_date","text":"","code":"expand_date(.data, col, text_months = FALSE)"},{"path":"/reference/expand_date.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Expand date-time column in a data frame — expand_date","text":".data data frame harp_list data frames col name date-time column expanded. Can quoted unquoted. using variable, wrapped . text_months Logical. TRUE, month names used rather numbers.","code":""},{"path":"/reference/expand_date.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Expand date-time column in a data frame — expand_date","text":"data frame harp_list data frames new columns expanded date","code":""},{"path":"/reference/expand_date.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Expand date-time column in a data frame — expand_date","text":"","code":"expand_date(det_point_df, fcst_dttm) #> ::deterministic point forecast:: # A tibble: 48 × 11 #>    fcst_model fcst_dttm           lead_time valid_dttm            SID  fcst #>    <chr>      <dttm>                  <dbl> <dttm>              <dbl> <dbl> #>  1 point      2021-01-01 00:00:00         0 2021-01-01 00:00:00  1001 0.300 #>  2 point      2021-01-01 00:00:00         1 2021-01-01 01:00:00  1001 0.611 #>  3 point      2021-01-01 00:00:00         2 2021-01-01 02:00:00  1001 0.802 #>  4 point      2021-01-01 00:00:00         3 2021-01-01 03:00:00  1001 0.361 #>  5 point      2021-01-01 00:00:00         4 2021-01-01 04:00:00  1001 0.213 #>  6 point      2021-01-01 00:00:00         5 2021-01-01 05:00:00  1001 0.736 #>  7 point      2021-01-01 00:00:00         6 2021-01-01 06:00:00  1001 0.177 #>  8 point      2021-01-01 00:00:00         7 2021-01-01 07:00:00  1001 0.866 #>  9 point      2021-01-01 00:00:00         8 2021-01-01 08:00:00  1001 0.109 #> 10 point      2021-01-01 00:00:00         9 2021-01-01 09:00:00  1001 0.436 #> # ℹ 38 more rows #> # ℹ 5 more variables: fcst_year <int>, fcst_month <int>, fcst_day <int>, #> #   fcst_hour <int>, fcst_minute <int> expand_date(det_point_list, valid_dttm) #> • a #> ::deterministic point forecast:: # A tibble: 48 × 11 #>    fcst_model fcst_dttm           lead_time valid_dttm            SID   fcst #>    <chr>      <dttm>                  <dbl> <dttm>              <dbl>  <dbl> #>  1 a          2021-01-01 00:00:00         0 2021-01-01 00:00:00  1001 0.254  #>  2 a          2021-01-01 00:00:00         1 2021-01-01 01:00:00  1001 0.0506 #>  3 a          2021-01-01 00:00:00         2 2021-01-01 02:00:00  1001 0.236  #>  4 a          2021-01-01 00:00:00         3 2021-01-01 03:00:00  1001 0.298  #>  5 a          2021-01-01 00:00:00         4 2021-01-01 04:00:00  1001 0.467  #>  6 a          2021-01-01 00:00:00         5 2021-01-01 05:00:00  1001 0.376  #>  7 a          2021-01-01 00:00:00         6 2021-01-01 06:00:00  1001 0.217  #>  8 a          2021-01-01 00:00:00         7 2021-01-01 07:00:00  1001 0.696  #>  9 a          2021-01-01 00:00:00         8 2021-01-01 08:00:00  1001 0.227  #> 10 a          2021-01-01 00:00:00         9 2021-01-01 09:00:00  1001 0.359  #> # ℹ 38 more rows #> # ℹ 5 more variables: valid_year <int>, valid_month <int>, valid_day <int>, #> #   valid_hour <int>, valid_minute <int> #>  #> • b #> ::deterministic point forecast:: # A tibble: 48 × 11 #>    fcst_model fcst_dttm           lead_time valid_dttm            SID  fcst #>    <chr>      <dttm>                  <dbl> <dttm>              <dbl> <dbl> #>  1 b          2021-01-01 00:00:00         0 2021-01-01 00:00:00  1001 0.746 #>  2 b          2021-01-01 00:00:00         1 2021-01-01 01:00:00  1001 0.409 #>  3 b          2021-01-01 00:00:00         2 2021-01-01 02:00:00  1001 0.484 #>  4 b          2021-01-01 00:00:00         3 2021-01-01 03:00:00  1001 0.677 #>  5 b          2021-01-01 00:00:00         4 2021-01-01 04:00:00  1001 0.730 #>  6 b          2021-01-01 00:00:00         5 2021-01-01 05:00:00  1001 0.413 #>  7 b          2021-01-01 00:00:00         6 2021-01-01 06:00:00  1001 0.689 #>  8 b          2021-01-01 00:00:00         7 2021-01-01 07:00:00  1001 0.430 #>  9 b          2021-01-01 00:00:00         8 2021-01-01 08:00:00  1001 0.720 #> 10 b          2021-01-01 00:00:00         9 2021-01-01 09:00:00  1001 0.194 #> # ℹ 38 more rows #> # ℹ 5 more variables: valid_year <int>, valid_month <int>, valid_day <int>, #> #   valid_hour <int>, valid_minute <int> #>  expand_date(ens_point_df, valid_dttm, text_months = TRUE) #> ::ensemble point forecast:: # A tibble: 48 × 11 #>    fcst_dttm           lead_time valid_dttm            SID point_mbr000 #>    <dttm>                  <dbl> <dttm>              <dbl>        <dbl> #>  1 2021-01-01 00:00:00         0 2021-01-01 00:00:00  1001       0.277  #>  2 2021-01-01 00:00:00         1 2021-01-01 01:00:00  1001       0.650  #>  3 2021-01-01 00:00:00         2 2021-01-01 02:00:00  1001       0.601  #>  4 2021-01-01 00:00:00         3 2021-01-01 03:00:00  1001       0.427  #>  5 2021-01-01 00:00:00         4 2021-01-01 04:00:00  1001       0.0798 #>  6 2021-01-01 00:00:00         5 2021-01-01 05:00:00  1001       0.762  #>  7 2021-01-01 00:00:00         6 2021-01-01 06:00:00  1001       0.347  #>  8 2021-01-01 00:00:00         7 2021-01-01 07:00:00  1001       0.488  #>  9 2021-01-01 00:00:00         8 2021-01-01 08:00:00  1001       0.438  #> 10 2021-01-01 00:00:00         9 2021-01-01 09:00:00  1001       0.662  #> # ℹ 38 more rows #> # ℹ 6 more variables: point_mbr001 <dbl>, valid_year <int>, valid_month <chr>, #> #   valid_day <int>, valid_hour <int>, valid_minute <int>  # Column name can be quoted expand_date(ens_grid_df, \"fcst_dttm\") #> ::ensemble gridded forecast:: # A tibble: 24 × 10 #>    fcst_dttm           lead_time valid_dttm          grid_mbr000 grid_mbr001 #>    <dttm>                  <dbl> <dttm>                <geolist>   <geolist> #>  1 2021-01-01 00:00:00         0 2021-01-01 00:00:00     [5 × 5]     [5 × 5] #>  2 2021-01-01 00:00:00         1 2021-01-01 01:00:00     [5 × 5]     [5 × 5] #>  3 2021-01-01 00:00:00         2 2021-01-01 02:00:00     [5 × 5]     [5 × 5] #>  4 2021-01-01 00:00:00         3 2021-01-01 03:00:00     [5 × 5]     [5 × 5] #>  5 2021-01-01 00:00:00         4 2021-01-01 04:00:00     [5 × 5]     [5 × 5] #>  6 2021-01-01 00:00:00         5 2021-01-01 05:00:00     [5 × 5]     [5 × 5] #>  7 2021-01-01 00:00:00         6 2021-01-01 06:00:00     [5 × 5]     [5 × 5] #>  8 2021-01-01 00:00:00         7 2021-01-01 07:00:00     [5 × 5]     [5 × 5] #>  9 2021-01-01 00:00:00         8 2021-01-01 08:00:00     [5 × 5]     [5 × 5] #> 10 2021-01-01 00:00:00         9 2021-01-01 09:00:00     [5 × 5]     [5 × 5] #> # ℹ 14 more rows #> # ℹ 5 more variables: fcst_year <int>, fcst_month <int>, fcst_day <int>, #> #   fcst_hour <int>, fcst_minute <int>  # If using a variable, wrap in {{<var>}} my_col <- \"fcst_dttm\" expand_date(ens_grid_df, {{my_col}}) #> ::ensemble gridded forecast:: # A tibble: 24 × 10 #>    fcst_dttm           lead_time valid_dttm          grid_mbr000 grid_mbr001 #>    <dttm>                  <dbl> <dttm>                <geolist>   <geolist> #>  1 2021-01-01 00:00:00         0 2021-01-01 00:00:00     [5 × 5]     [5 × 5] #>  2 2021-01-01 00:00:00         1 2021-01-01 01:00:00     [5 × 5]     [5 × 5] #>  3 2021-01-01 00:00:00         2 2021-01-01 02:00:00     [5 × 5]     [5 × 5] #>  4 2021-01-01 00:00:00         3 2021-01-01 03:00:00     [5 × 5]     [5 × 5] #>  5 2021-01-01 00:00:00         4 2021-01-01 04:00:00     [5 × 5]     [5 × 5] #>  6 2021-01-01 00:00:00         5 2021-01-01 05:00:00     [5 × 5]     [5 × 5] #>  7 2021-01-01 00:00:00         6 2021-01-01 06:00:00     [5 × 5]     [5 × 5] #>  8 2021-01-01 00:00:00         7 2021-01-01 07:00:00     [5 × 5]     [5 × 5] #>  9 2021-01-01 00:00:00         8 2021-01-01 08:00:00     [5 × 5]     [5 × 5] #> 10 2021-01-01 00:00:00         9 2021-01-01 09:00:00     [5 × 5]     [5 × 5] #> # ℹ 14 more rows #> # ℹ 5 more variables: fcst_year <int>, fcst_month <int>, fcst_day <int>, #> #   fcst_hour <int>, fcst_minute <int>"},{"path":"/reference/extract_numeric.html","id":null,"dir":"Reference","previous_headings":"","what":"Extract numeric values from a string or vector of strings — extract_numeric","title":"Extract numeric values from a string or vector of strings — extract_numeric","text":"string contains characters numbers, extract_numeric extract numeric parts. Anything numerics string result separate element output.","code":""},{"path":"/reference/extract_numeric.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Extract numeric values from a string or vector of strings — extract_numeric","text":"","code":"extract_numeric(x, as_list = FALSE)"},{"path":"/reference/extract_numeric.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Extract numeric values from a string or vector of strings — extract_numeric","text":"x character vector. as_list Logical. Whether return list. See Details.","code":""},{"path":"/reference/extract_numeric.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Extract numeric values from a string or vector of strings — extract_numeric","text":"vector list numerics","code":""},{"path":"/reference/extract_numeric.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Extract numeric values from a string or vector of strings — extract_numeric","text":"vector strings passed want return something length, set delist = FALSE list returned length input vector.","code":""},{"path":"/reference/extract_numeric.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Extract numeric values from a string or vector of strings — extract_numeric","text":"","code":"extract_numeric(\"3h\") #> [1] 3  # With decimals extract_numeric(\"3h 4h 5.5h\") #> [1] 3.0 4.0 5.5  # With negative values extract_numeric(\"-2s4s 6s\") #> [1] -2  4  6  # With a vector of values extract_numeric(c(\"2h\", \"3h\", \"7h\")) #> [1] 2 3 7 extract_numeric(c(\"2h\", \"3h 4h 5h 6h\", \"7h\")) #> [1] 2 3 4 5 6 7  # Return a list of the same length as the vector extract_numeric(c(\"2h\", \"3h 4h 5h 6h\", \"7h\"), as_list = TRUE) #> [[1]] #> [1] 2 #>  #> [[2]] #> [1] 3 4 5 6 #>  #> [[3]] #> [1] 7 #>"},{"path":"/reference/filter-joins-harp-list.html","id":null,"dir":"Reference","previous_headings":"","what":"Filtering joins for harp_lists — filter-joins-harp-list","title":"Filtering joins for harp_lists — filter-joins-harp-list","text":"harp_list methods filtering join functions, e.g. semi_join dplyr package. case x harp_list, y can either data frame another harp_list length x. y data frame, attempt made join y data frame harp_list, x. y harp_list, data frames corresponding elements x y joined. Note done basis location list names taken account. Names output always taken names x.","code":""},{"path":"/reference/filter-joins-harp-list.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Filtering joins for harp_lists — filter-joins-harp-list","text":"","code":"# S3 method for harp_list semi_join(x, y, by = NULL, ...)  # S3 method for harp_list anti_join(x, y, by = NULL, ...)"},{"path":"/reference/filter-joins-harp-list.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Filtering joins for harp_lists — filter-joins-harp-list","text":"x, y pair data frames, data frame extensions (e.g. tibble), lazy data frames (e.g. dbplyr dtplyr). See Methods, , details. join specification created join_by(), character vector variables join . NULL, default, *_join() perform natural join, using variables common across x y. message lists variables can check correct; suppress message supplying explicitly. join different variables x y, use join_by() specification. example, join_by(== b) match x$y$b. join multiple variables, use join_by() specification multiple expressions. example, join_by(== b, c == d) match x$y$b x$c y$d. column names x y, can shorten listing variable names, like join_by(, c). join_by() can also used perform inequality, rolling, overlap joins. See documentation ?join_by details types joins. simple equality joins, can alternatively specify character vector variable names join . example, = c(\"\", \"b\") joins x$y$x$b y$b. variable names differ x y, use named character vector like = c(\"x_a\" = \"y_a\", \"x_b\" = \"y_b\"). perform cross-join, generating combinations x y, see cross_join(). ... Arguments passed dplyr::semi_join, dplyr::anti_join","code":""},{"path":"/reference/filter-joins-harp-list.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Filtering joins for harp_lists — filter-joins-harp-list","text":"harp_list names x.","code":""},{"path":"/reference/filter.harp_list.html","id":null,"dir":"Reference","previous_headings":"","what":"Subset rows using column values — filter.harp_list","title":"Subset rows using column values — filter.harp_list","text":"harp_list method filter(). works exactly way except harp_list returned. use function, dplyr package must attached .harp_list suffix can dropped.","code":""},{"path":"/reference/filter.harp_list.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Subset rows using column values — filter.harp_list","text":"","code":"# S3 method for harp_list filter(.data, ...)"},{"path":"/reference/filter.harp_list.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Subset rows using column values — filter.harp_list","text":".data data frame, data frame extension (e.g. tibble), lazy data frame (e.g. dbplyr dtplyr). See Methods, , details. ... <data-masking> Expressions return logical value, defined terms variables .data. multiple expressions included, combined & operator. rows conditions evaluate TRUE kept.","code":""},{"path":"/reference/geo_opts.html","id":null,"dir":"Reference","previous_headings":"","what":"Options for different transformations — geo_opts","title":"Options for different transformations — geo_opts","text":"using geo_transform transformation options must passed named list appropriate transformation. functions used generate named lists.","code":""},{"path":"/reference/geo_opts.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Options for different transformations — geo_opts","text":"","code":"geo_opts_points(   points,   method = c(\"bilinear\", \"nearest\", \"bicubic\"),   mask = NULL,   force = FALSE,   weights = NULL,   keep_weights = FALSE )  geo_opts_regrid(   new_grid,   method = c(\"bilinear\", \"nearest\", \"bicubic\"),   mask = NULL,   new_mask = NULL,   weights = NULL,   keep_weights = FALSE )  geo_opts_subgrid(i1, i2, j1, j2)  geo_opts_zoom(centre_lon, centre_lat, length_x, length_y)  geo_opts_xsection(   p1,   p2,   n = 100,   method = c(\"bilinear\", \"nearest\", \"bicubic\"),   weights = NULL,   keep_weights = FALSE )  geo_opts_upscale(factor, method = \"mean\", downsample_location = \"bottom_left\")"},{"path":"/reference/geo_opts.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Options for different transformations — geo_opts","text":"points data frame geographic points interpolate gridded data. data frame must include columns \"SID\" unique id point, \"lon\" longitude point decimal degrees \"lat\" latitude point decimal degrees. data frame can contain columns, retained output. method interpolation method. Can \"nearest\" nearest neighbour, \"bilinear\", \"bicubic.\" default \"bilinear\". geo_upscale, can function summarises vector single value can found match.fun, default \"mean\". option \"downsample\", dwhich described argument downsample_location. mask mask prevent grid points used interpolation. grid x grid points values 0 FALSE masked interpolation. force interpolations include mask, possible point surrounded 4 masked points. case mask ignored 4 points used interpolation (default). Set force = TRUE force mask applied set interpolated NA. weights Pre-computed weights interpolation. output appropriate geo_weights function. keep_weights Whether keep weights output. set TRUE, return object \"weights\" attribute. new_grid geofield geodomain grid x regridded . define_domain can used define new geodomain. new_mask geofield grid new_grid grid points interpolated set 0 FALSE. i1 x index western side sub domain. i2 x index eastern side sub domain. j1 y index southern side sub domain. j2 y index northern side sub domain. centre_lon longitude decimal degrees centre zoomed grid. centre_lat latitude decimal degrees centre zoomed grid. length_x number grid squares west east zoomed grid. even number used, extended 1 since zoomed grid centred grid square containing (centre_lat, centre_lon). length_y number grid squares south north zoomed grid. even number used, extended 1 since zoomed grid centred grid square containing (centre_lat, centre_lon). p1 geographic location decimal degrees start section. vector length 2 first value longitude second value latitude. p2 geographic location decimal degrees end section. vector length 2 first value longitude second value latitude. n number equally spaced points along section. default 100. factor integer upscale data. Can length 2 achieve different upscaling x directions. downsample_location \"downsample\" chosen method, pixel upscaled field sampled pixel original field inside upscaled pixel. location pixel can one \"bottom_left\", \"bottom_centre\", \"bottom_right\", \"left_centre\", \"centre\", \"right_centre\", \"top_right\", \"top_centre\", \"top_left\" \"random\".","code":""},{"path":"/reference/geo_transform.html","id":null,"dir":"Reference","previous_headings":"","what":"Geographic transformation of gridded data — geo_points","title":"Geographic transformation of gridded data — geo_points","text":"Gridded data can transformed one grid definition another, geographic points, cross sections, sub-domains original grid, zoomed original grid. geo_<transformation> functions used achieve , generalized function geo_transform designed used functions take transformation argument.","code":""},{"path":"/reference/geo_transform.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Geographic transformation of gridded data — geo_points","text":"","code":"geo_points(   x,   points,   method = c(\"bilinear\", \"nearest\", \"bicubic\"),   mask = NULL,   force = FALSE,   weights = NULL,   keep_weights = FALSE )  geo_regrid(   x,   new_grid,   method = c(\"bilinear\", \"nearest\", \"bicubic\"),   mask = NULL,   new_mask = NULL,   weights = NULL,   keep_weights = FALSE )  geo_subgrid(x, i1, i2, j1, j2)  geo_zoom(x, centre_lon, centre_lat, length_x, length_y)  geo_xsection(   x,   p1,   p2,   n = 100,   method = c(\"bilinear\", \"nearest\", \"bicubic\"),   weights = NULL,   keep_weights = FALSE )  geo_upscale(   x,   factor,   method = \"mean\",   downsample_location = \"bottom_left\",   ... )  geo_transform(   x,   trans = c(\"points\", \"regrid\", \"subgrid\", \"zoom\", \"xsection\"),   opts )"},{"path":"/reference/geo_transform.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Geographic transformation of gridded data — geo_points","text":"x geofield, geolist, data frame class harp_grid_df. transformations involve interpolation gridded data (e.g. zoom, subgrid) x can also geodomain. points data frame geographic points interpolate gridded data. data frame must include columns \"SID\" unique id point, \"lon\" longitude point decimal degrees \"lat\" latitude point decimal degrees. data frame can contain columns, retained output. method interpolation method. Can \"nearest\" nearest neighbour, \"bilinear\", \"bicubic.\" default \"bilinear\". geo_upscale, can function summarises vector single value can found match.fun, default \"mean\". option \"downsample\", dwhich described argument downsample_location. mask mask prevent grid points used interpolation. grid x grid points values 0 FALSE masked interpolation. force interpolations include mask, possible point surrounded 4 masked points. case mask ignored 4 points used interpolation (default). Set force = TRUE force mask applied set interpolated NA. weights Pre-computed weights interpolation. output appropriate geo_weights function. keep_weights Whether keep weights output. set TRUE, return object \"weights\" attribute. new_grid geofield geodomain grid x regridded . define_domain can used define new geodomain. new_mask geofield grid new_grid grid points interpolated set 0 FALSE. i1 x index western side sub domain. i2 x index eastern side sub domain. j1 y index southern side sub domain. j2 y index northern side sub domain. centre_lon longitude decimal degrees centre zoomed grid. centre_lat latitude decimal degrees centre zoomed grid. length_x number grid squares west east zoomed grid. even number used, extended 1 since zoomed grid centred grid square containing (centre_lat, centre_lon). length_y number grid squares south north zoomed grid. even number used, extended 1 since zoomed grid centred grid square containing (centre_lat, centre_lon). p1 geographic location decimal degrees start section. vector length 2 first value longitude second value latitude. p2 geographic location decimal degrees end section. vector length 2 first value longitude second value latitude. n number equally spaced points along section. default 100. factor integer upscale data. Can length 2 achieve different upscaling x directions. downsample_location \"downsample\" chosen method, pixel upscaled field sampled pixel original field inside upscaled pixel. location pixel can one \"bottom_left\", \"bottom_centre\", \"bottom_right\", \"left_centre\", \"centre\", \"right_centre\", \"top_right\", \"top_centre\", \"top_left\" \"random\". ... Extra options method. trans transformation apply. Can \"points\", \"regrid\", \"xsection\", \"subgrid\", \"zoom\". opts list options chosen transformation. appropriate geo_opts function used generate list.","code":""},{"path":"/reference/geo_transform.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Geographic transformation of gridded data — geo_points","text":"case transformations points cross sections, data frame. cases object class x transformation applied.","code":""},{"path":"/reference/geo_transform.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Geographic transformation of gridded data — geo_points","text":"geo_points used interpolate regular grid geographic points within domain grid geo_regrid used interpolate one regular grid another regular grid. can include reprojection one grid projection another. geo_xsection extracts equally spaced straight line points geographic locations cen used construct vertical cross section 3-dimensional field. grids equally space longitude - latitude coordinates (e.g. latlong projections) section can along great circle thus shortest distance two points. geo_subgrid extracts sub domain data without changing coordinate reference system. geo_zoom special case geo_subgrid whereby sub domain data extracted centred around geographic point. geo_upscale upscales data higher resolution grid coarser resolution grid using integer upscaling factor. default method take mean high resolution pixels inside coarse resolution pixels, though sampling using \"downsample\" method faster likely sufficient upscaling raster raster plotting. geo_transform generalized function can used functions take type geographic transformation argument. transformations require interpolation data (points, regrid xsection), method interpolation can chosen. available interpolation methods nearest neighbour, bilinear bicubic. addition, masks can used prevent grid points used interpolation - example land-sea mask, grid points value 0 FALSE used interpolation.","code":""},{"path":"/reference/geo_weights.html","id":null,"dir":"Reference","previous_headings":"","what":"Compute interpolation weights for geographic transformations — geo_weights","title":"Compute interpolation weights for geographic transformations — geo_weights","text":"repeated transformations grid definition, may efficient compute interpolation weights first compute transformations weights. functions used compute interpolation weights, can passed geo_transform function via opts argument, explicit geo_<transformation> function via weights argument","code":""},{"path":"/reference/geo_weights.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Compute interpolation weights for geographic transformations — geo_weights","text":"","code":"geo_weights(x, trans = c(\"points\", \"regrid\", \"xsection\"), opts)  geo_weights_points(   x,   points,   method = c(\"bilinear\", \"nearest\", \"bicubic\"),   mask = NULL,   force = FALSE )  geo_weights_regrid(   x,   new_grid,   method = c(\"bilinear\", \"nearest\", \"bicubic\"),   mask = NULL,   new_mask = NULL )  geo_weights_xsection(   x,   p1,   p2,   n = 100,   method = c(\"bilinear\", \"nearest\", \"bicubic\") )"},{"path":"/reference/geo_weights.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Compute interpolation weights for geographic transformations — geo_weights","text":"x geofield, geolist, data frame class harp_grid_df. transformations involve interpolation gridded data (e.g. zoom, subgrid) x can also geodomain. trans transformation apply. Can \"points\", \"regrid\", \"xsection\", \"subgrid\", \"zoom\". opts list options chosen transformation. appropriate geo_opts function used generate list. points data frame geographic points interpolate gridded data. data frame must include columns \"SID\" unique id point, \"lon\" longitude point decimal degrees \"lat\" latitude point decimal degrees. data frame can contain columns, retained output. method interpolation method. Can \"nearest\" nearest neighbour, \"bilinear\", \"bicubic.\" default \"bilinear\". geo_upscale, can function summarises vector single value can found match.fun, default \"mean\". option \"downsample\", dwhich described argument downsample_location. mask mask prevent grid points used interpolation. grid x grid points values 0 FALSE masked interpolation. force interpolations include mask, possible point surrounded 4 masked points. case mask ignored 4 points used interpolation (default). Set force = TRUE force mask applied set interpolated NA. new_grid geofield geodomain grid x regridded . define_domain can used define new geodomain. new_mask geofield grid new_grid grid points interpolated set 0 FALSE. p1 geographic location decimal degrees start section. vector length 2 first value longitude second value latitude. p2 geographic location decimal degrees end section. vector length 2 first value longitude second value latitude. n number equally spaced points along section. default 100.","code":""},{"path":"/reference/geo_weights.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Compute interpolation weights for geographic transformations — geo_weights","text":"interpolation weights geographic transformation.","code":""},{"path":"/reference/geofield.html","id":null,"dir":"Reference","previous_headings":"","what":"Geofields — geofield","title":"Geofields — geofield","text":"geofield georeferenced 2d-array. coordinate reference system stored attributes array. Used together define_domain, geofield can created 2d-array. Additionally, geofield() can used extract geofield geolist. can useful extracting geofield pipeline.","code":""},{"path":"/reference/geofield.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Geofields — geofield","text":"","code":"geofield(x, ...)  # S3 method for harp_geolist geofield(x, i = 1, ...)  # S3 method for array geofield(x, domain, ...)"},{"path":"/reference/geofield.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Geofields — geofield","text":"x 2d array geolist ... Used methods element geolist extract. domain geodomain dimensions x.","code":""},{"path":"/reference/geofield.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Geofields — geofield","text":"geofield.","code":""},{"path":"/reference/geofield.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Geofields — geofield","text":"","code":"my_domain <- define_domain(10, 60, 300, 10000) gfld <- geofield(   array(rnorm(300 * 300), c(300, 300)),   domain = my_domain ) meteogrid::iview(gfld)   geofield(det_grid_df$fcst, 4) #>  :   #> Time: #>  NULL #> Domain summary: #> 5 x 5 domain #> Projection summary: #> proj= lcc  #> centre = ( 10.74 , 59.91 ) #> Data summary: #> 0.02995663 0.3776416 0.5347816 0.5381101 0.6854759 0.9864545  # is equivalent to det_grid_df$fcst[[4]] #>  :   #> Time: #>  NULL #> Domain summary: #> 5 x 5 domain #> Projection summary: #> proj= lcc  #> centre = ( 10.74 , 59.91 ) #> Data summary: #> 0.02995663 0.3776416 0.5347816 0.5381101 0.6854759 0.9864545  # but geofield() can be used in a pipeline det_grid_df$fcst %>%   geofield(4) #>  :   #> Time: #>  NULL #> Domain summary: #> 5 x 5 domain #> Projection summary: #> proj= lcc  #> centre = ( 10.74 , 59.91 ) #> Data summary: #> 0.02995663 0.3776416 0.5347816 0.5381101 0.6854759 0.9864545"},{"path":"/reference/geolist.html","id":null,"dir":"Reference","previous_headings":"","what":"Geolists — geolist","title":"Geolists — geolist","text":"geolist list geofields domain. main purpose geolist used column data frame.","code":""},{"path":"/reference/geolist.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Geolists — geolist","text":"","code":"geolist(..., domain = NULL)  is_geolist(x)  variance(x, na.rm = FALSE, ...)  std_dev(x, na.rm = FALSE, ...)"},{"path":"/reference/geolist.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Geolists — geolist","text":"... geofields list containing geofields domain Typically used, since domain obtained geofields x object na.rm Logical. Whether remove NAs calculation.","code":""},{"path":"/reference/geolist.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Geolists — geolist","text":"list geofields domain class harp_geolist","code":""},{"path":"/reference/geolist.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Geolists — geolist","text":"number methods available geolists reduce geolist geolist containing single geofield. include: mean - element-wise mean geolist std_dev - element-wise standard deviation geolist variance element-wise variance geolist min - element-wise minimum geolist max - element-wise maximum geolist cumsum - element-wise cumulative sum geolist cumprod - element-wise cumulative product geolist cummin - element-wise cumulative minimum geolist cummax - element-wise cumulative maximum geolist - element-wise logical geolist TRUE - element-wise logical geolist TRUE Note R stats functions sd var used geolists since implemented methods, std_dev variance must used instead. addition R's generic math logic functions work geolists. is_geolist() checks argument valid geolist.","code":""},{"path":"/reference/geolist.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Geolists — geolist","text":"","code":"# Define a domain my_domain <- define_domain(10, 60, 300, 10000)  # geolist from indivdual geofields geolist(   geofield(array(rnorm(300 * 300), c(300, 300)), domain = my_domain),   geofield(array(rnorm(300 * 300), c(300, 300)), domain = my_domain) ) #> <harp_geolist[2]> #> [[1]] <numeric geofield [300 x 300] Min = -4.111 Max = 4.528 Mean = -0.004> #> [[2]] <numeric geofield [300 x 300] Min = -4.041 Max = 3.990 Mean = 0.002>  # geolist from a list of geofields gfld <- lapply(   1:10,   function(x) geofield(     array(runif(300 * 300), c(300, 300)), domain = my_domain   ) ) glst <- geolist(gfld) glst #> <harp_geolist[10]> #>  [[1]] <numeric geofield [300 x 300] Min = 0.000 Max = 1.000 Mean = 0.499> #>  [[2]] <numeric geofield [300 x 300] Min = 0.000 Max = 1.000 Mean = 0.499> #>  [[3]] <numeric geofield [300 x 300] Min = 0.000 Max = 1.000 Mean = 0.501> #>  [[4]] <numeric geofield [300 x 300] Min = 0.000 Max = 1.000 Mean = 0.501> #>  [[5]] <numeric geofield [300 x 300] Min = 0.000 Max = 1.000 Mean = 0.499> #>  [[6]] <numeric geofield [300 x 300] Min = 0.000 Max = 1.000 Mean = 0.502> #>  [[7]] <numeric geofield [300 x 300] Min = 0.000 Max = 1.000 Mean = 0.501> #>  [[8]] <numeric geofield [300 x 300] Min = 0.000 Max = 1.000 Mean = 0.501> #>  [[9]] <numeric geofield [300 x 300] Min = 0.000 Max = 1.000 Mean = 0.500> #> [[10]] <numeric geofield [300 x 300] Min = 0.000 Max = 1.000 Mean = 0.500>  # Summarise geolist to a single geofield mean(glst) #> <harp_geolist[1]> #> [[1]] <numeric geofield [300 x 300] Min = 0.163 Max = 0.851 Mean = 0.500> std_dev(glst) #> <harp_geolist[1]> #> [[1]] <numeric geofield [300 x 300] Min = 0.076 Max = 0.455 Mean = 0.285> variance(glst) #> <harp_geolist[1]> #> [[1]] <numeric geofield [300 x 300] Min = 0.006 Max = 0.207 Mean = 0.084> min(glst) #> <harp_geolist[1]> #> [[1]] <numeric geofield [300 x 300] Min = 0.000 Max = 0.671 Mean = 0.091> max(glst) #> <harp_geolist[1]> #> [[1]] <numeric geofield [300 x 300] Min = 0.285 Max = 1.000 Mean = 0.910> any(glst > 0.9) #> <harp_geolist[1]> #> [[1]] <logical geofield [300 x 300] TRUE: 58950 FALSE: 31050> all(glst > 0.25) #> <harp_geolist[1]> #> [[1]] <logical geofield [300 x 300] TRUE: 5082 FALSE: 84918>"},{"path":"/reference/get_domain.html","id":null,"dir":"Reference","previous_headings":"","what":"Extract domain information as a geodomain object — get_domain","title":"Extract domain information as a geodomain object — get_domain","text":"\"domain\" attribute extracted input returned geodomain.","code":""},{"path":"/reference/get_domain.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Extract domain information as a geodomain object — get_domain","text":"","code":"get_domain(x)"},{"path":"/reference/get_domain.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Extract domain information as a geodomain object — get_domain","text":"x geofield geolist","code":""},{"path":"/reference/get_domain.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Extract domain information as a geodomain object — get_domain","text":"geodomain","code":""},{"path":"/reference/get_domain.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Extract domain information as a geodomain object — get_domain","text":"","code":"get_domain(det_grid_df$fcst) #> 5 x 5 domain #> Projection summary: #> proj= lcc  #> centre = ( 10.74 , 59.91 )  get_domain(det_grid_df$fcst[[1]]) #> 5 x 5 domain #> Projection summary: #> proj= lcc  #> centre = ( 10.74 , 59.91 )"},{"path":"/reference/glapply.html","id":null,"dir":"Reference","previous_headings":"","what":"Apply a function to each element of a geolist — glapply","title":"Apply a function to each element of a geolist — glapply","text":"glapply implementation lapply especially geolists. FUN applied element geolist geolist length returned.","code":""},{"path":"/reference/glapply.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Apply a function to each element of a geolist — glapply","text":"","code":"glapply(X, FUN, ...)"},{"path":"/reference/glapply.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Apply a function to each element of a geolist — glapply","text":"X vector (atomic list) expression     object.  objects (including classed objects) coerced     base::.list. FUN function applied element X:     see ‘Details’.  case functions like     +, %*%, function name must backquoted quoted. ... optional arguments FUN.","code":""},{"path":"/reference/glapply.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Apply a function to each element of a geolist — glapply","text":"geolist length X FUN applied elements.","code":""},{"path":"/reference/glapply.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Apply a function to each element of a geolist — glapply","text":"","code":"glapply(det_grid_df$fcst, sqrt) #> <harp_geolist[24]> #>  [[1]] <numeric geofield [5 x 5] Min = 0.132 Max = 0.988 Mean = 0.634> #>  [[2]] <numeric geofield [5 x 5] Min = 0.146 Max = 0.980 Mean = 0.599> #>  [[3]] <numeric geofield [5 x 5] Min = 0.246 Max = 0.932 Mean = 0.619> #>  [[4]] <numeric geofield [5 x 5] Min = 0.173 Max = 0.993 Mean = 0.699> #>  [[5]] <numeric geofield [5 x 5] Min = 0.025 Max = 0.993 Mean = 0.669> #>  [[6]] <numeric geofield [5 x 5] Min = 0.037 Max = 0.944 Mean = 0.662> #>  [[7]] <numeric geofield [5 x 5] Min = 0.058 Max = 0.992 Mean = 0.636> #>  [[8]] <numeric geofield [5 x 5] Min = 0.321 Max = 0.987 Mean = 0.704> #>  [[9]] <numeric geofield [5 x 5] Min = 0.039 Max = 0.984 Mean = 0.627> #> [[10]] <numeric geofield [5 x 5] Min = 0.195 Max = 0.965 Mean = 0.657> #> # 14 more geofields #> # Use `print(n = ...)` to see more #>   glapply(det_grid_df$fcst, function(x) x + 10) #> <harp_geolist[24]> #>  [[1]] <numeric geofield [5 x 5] Min = 10.018 Max = 10.976 Mean = 10.460> #>  [[2]] <numeric geofield [5 x 5] Min = 10.021 Max = 10.960 Mean = 10.418> #>  [[3]] <numeric geofield [5 x 5] Min = 10.060 Max = 10.868 Mean = 10.422> #>  [[4]] <numeric geofield [5 x 5] Min = 10.030 Max = 10.986 Mean = 10.538> #>  [[5]] <numeric geofield [5 x 5] Min = 10.001 Max = 10.985 Mean = 10.514> #>  [[6]] <numeric geofield [5 x 5] Min = 10.001 Max = 10.891 Mean = 10.492> #>  [[7]] <numeric geofield [5 x 5] Min = 10.003 Max = 10.984 Mean = 10.472> #>  [[8]] <numeric geofield [5 x 5] Min = 10.103 Max = 10.974 Mean = 10.531> #>  [[9]] <numeric geofield [5 x 5] Min = 10.002 Max = 10.969 Mean = 10.467> #> [[10]] <numeric geofield [5 x 5] Min = 10.038 Max = 10.930 Mean = 10.492> #> # 14 more geofields #> # Use `print(n = ...)` to see more #>"},{"path":"/reference/glapply2.html","id":null,"dir":"Reference","previous_headings":"","what":"Apply a function to element pairs of 2 geolists — glapply2","title":"Apply a function to element pairs of 2 geolists — glapply2","text":"glapply2() variant glapply() whereby function takes 2 geofields arguments applied element-wise 2 geolists. equivalent map2, explicitly geolists.","code":""},{"path":"/reference/glapply2.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Apply a function to element pairs of 2 geolists — glapply2","text":"","code":"glapply2(X, Y, FUN, ...)"},{"path":"/reference/glapply2.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Apply a function to element pairs of 2 geolists — glapply2","text":"X geolist Y geolist length X domain. FUN function apply X Y. ... arguments FUN.","code":""},{"path":"/reference/glapply2.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Apply a function to element pairs of 2 geolists — glapply2","text":"geolist length X Y.","code":""},{"path":"/reference/glapply2.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Apply a function to element pairs of 2 geolists — glapply2","text":"","code":"glapply2(   ens_grid_df$grid_mbr000, ens_grid_df$grid_mbr001, function(x, y) x + y ) #> <harp_geolist[24]> #>  [[1]] <numeric geofield [5 x 5] Min = 0.180 Max = 1.843 Mean = 0.907> #>  [[2]] <numeric geofield [5 x 5] Min = 0.301 Max = 1.743 Mean = 1.142> #>  [[3]] <numeric geofield [5 x 5] Min = 0.498 Max = 1.820 Mean = 1.073> #>  [[4]] <numeric geofield [5 x 5] Min = 0.124 Max = 1.788 Mean = 0.978> #>  [[5]] <numeric geofield [5 x 5] Min = 0.242 Max = 1.628 Mean = 1.021> #>  [[6]] <numeric geofield [5 x 5] Min = 0.168 Max = 1.745 Mean = 1.030> #>  [[7]] <numeric geofield [5 x 5] Min = 0.204 Max = 1.862 Mean = 0.925> #>  [[8]] <numeric geofield [5 x 5] Min = 0.084 Max = 1.909 Mean = 0.927> #>  [[9]] <numeric geofield [5 x 5] Min = 0.225 Max = 1.807 Mean = 1.067> #> [[10]] <numeric geofield [5 x 5] Min = 0.085 Max = 1.520 Mean = 0.886> #> # 14 more geofields #> # Use `print(n = ...)` to see more #>"},{"path":"/reference/harp-geolist.html","id":null,"dir":"Reference","previous_headings":"","what":"Internal vctrs methods — harp-geolist","title":"Internal vctrs methods — harp-geolist","text":"Internal vctrs methods","code":""},{"path":"/reference/join_to_fcst.html","id":null,"dir":"Reference","previous_headings":"","what":"Join data to a forecast — join_to_fcst","title":"Join data to a forecast — join_to_fcst","text":"join_to_fcst special case join family functions. primary purpose join data frame observations data frame harp_list forecasts forecast - observation pairs row joined data frame. extra check made make sure forecast data observations data units.","code":""},{"path":"/reference/join_to_fcst.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Join data to a forecast — join_to_fcst","text":"","code":"join_to_fcst(   .fcst,   .join,   join_type = c(\"inner\", \"left\", \"right\", \"full\", \"semi\", \"anti\"),   by = NULL,   latlon = FALSE,   elev = FALSE,   force = FALSE,   keep_x = TRUE,   keep_y = FALSE,   ... )"},{"path":"/reference/join_to_fcst.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Join data to a forecast — join_to_fcst","text":".fcst harp_df data frame harp_list. .join data frame join forecast. join_type join data frame. Acceptable values : \"inner\", \"left\", \"right\", \"full\", \"semi\", \"anti\". See join details. columns join - set NULL natural join done, using variables common names across .fcst .join. default join using common columns .fcst .join excluding lat, lon elev. may stored different levels precision join thus fail. latlon Logical. Whether include latitude longitude columns default . default FALSE. elev Logical. Whether include station elevation column default . default FALSE. force Set TRUE force join happen even units .fcst .join compatible. keep_x, keep_y duplicate column names found, used join, arguments used indicate whether duplicate columns .fcst (keep_x), .join (keep_y) kept. default keep_x = TRUE, keep_y = FALSE. ... arguments join.","code":""},{"path":"/reference/join_to_fcst.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Join data to a forecast — join_to_fcst","text":"input forecast data frame column(s) added .join.","code":""},{"path":"/reference/join_to_fcst.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Join data to a forecast — join_to_fcst","text":"","code":"# Make some fake observations library(tibble) obs <- tibble(   valid_dttm = det_point_df$valid_dttm,   SID       = det_point_df$SID,   units     = \"degC\",   T2m       = runif(nrow(det_point_df)) )  # Make sure the forecast has units fcst <- set_units(det_point_df, \"degC\")  join_to_fcst(fcst, obs) #> Joining, by = c(\"valid_dttm\", \"SID\", \"units\") #> ::deterministic point forecast:: # A tibble: 48 × 8 #>    fcst_model fcst_dttm           lead_time valid_dttm            SID  fcst #>    <chr>      <dttm>                  <dbl> <dttm>              <dbl> <dbl> #>  1 point      2021-01-01 00:00:00         0 2021-01-01 00:00:00  1001 0.300 #>  2 point      2021-01-01 00:00:00         1 2021-01-01 01:00:00  1001 0.611 #>  3 point      2021-01-01 00:00:00         2 2021-01-01 02:00:00  1001 0.802 #>  4 point      2021-01-01 00:00:00         3 2021-01-01 03:00:00  1001 0.361 #>  5 point      2021-01-01 00:00:00         4 2021-01-01 04:00:00  1001 0.213 #>  6 point      2021-01-01 00:00:00         5 2021-01-01 05:00:00  1001 0.736 #>  7 point      2021-01-01 00:00:00         6 2021-01-01 06:00:00  1001 0.177 #>  8 point      2021-01-01 00:00:00         7 2021-01-01 07:00:00  1001 0.866 #>  9 point      2021-01-01 00:00:00         8 2021-01-01 08:00:00  1001 0.109 #> 10 point      2021-01-01 00:00:00         9 2021-01-01 09:00:00  1001 0.436 #> # ℹ 38 more rows #> # ℹ 2 more variables: units <chr>, T2m <dbl>  # Also works for harp_list objects join_to_fcst(set_units(det_point_list, \"degC\"), obs) #> Joining, by = c(\"valid_dttm\", \"SID\", \"units\") #> Joining, by = c(\"valid_dttm\", \"SID\", \"units\") #> • a #> ::deterministic point forecast:: # A tibble: 48 × 8 #>    fcst_model fcst_dttm           lead_time valid_dttm            SID   fcst #>    <chr>      <dttm>                  <dbl> <dttm>              <dbl>  <dbl> #>  1 a          2021-01-01 00:00:00         0 2021-01-01 00:00:00  1001 0.254  #>  2 a          2021-01-01 00:00:00         1 2021-01-01 01:00:00  1001 0.0506 #>  3 a          2021-01-01 00:00:00         2 2021-01-01 02:00:00  1001 0.236  #>  4 a          2021-01-01 00:00:00         3 2021-01-01 03:00:00  1001 0.298  #>  5 a          2021-01-01 00:00:00         4 2021-01-01 04:00:00  1001 0.467  #>  6 a          2021-01-01 00:00:00         5 2021-01-01 05:00:00  1001 0.376  #>  7 a          2021-01-01 00:00:00         6 2021-01-01 06:00:00  1001 0.217  #>  8 a          2021-01-01 00:00:00         7 2021-01-01 07:00:00  1001 0.696  #>  9 a          2021-01-01 00:00:00         8 2021-01-01 08:00:00  1001 0.227  #> 10 a          2021-01-01 00:00:00         9 2021-01-01 09:00:00  1001 0.359  #> # ℹ 38 more rows #> # ℹ 2 more variables: units <chr>, T2m <dbl> #>  #> • b #> ::deterministic point forecast:: # A tibble: 48 × 8 #>    fcst_model fcst_dttm           lead_time valid_dttm            SID  fcst #>    <chr>      <dttm>                  <dbl> <dttm>              <dbl> <dbl> #>  1 b          2021-01-01 00:00:00         0 2021-01-01 00:00:00  1001 0.746 #>  2 b          2021-01-01 00:00:00         1 2021-01-01 01:00:00  1001 0.409 #>  3 b          2021-01-01 00:00:00         2 2021-01-01 02:00:00  1001 0.484 #>  4 b          2021-01-01 00:00:00         3 2021-01-01 03:00:00  1001 0.677 #>  5 b          2021-01-01 00:00:00         4 2021-01-01 04:00:00  1001 0.730 #>  6 b          2021-01-01 00:00:00         5 2021-01-01 05:00:00  1001 0.413 #>  7 b          2021-01-01 00:00:00         6 2021-01-01 06:00:00  1001 0.689 #>  8 b          2021-01-01 00:00:00         7 2021-01-01 07:00:00  1001 0.430 #>  9 b          2021-01-01 00:00:00         8 2021-01-01 08:00:00  1001 0.720 #> 10 b          2021-01-01 00:00:00         9 2021-01-01 09:00:00  1001 0.194 #> # ℹ 38 more rows #> # ℹ 2 more variables: units <chr>, T2m <dbl> #>   # And works with gridded data join_to_fcst(set_units(ens_grid_df, \"degC\"), set_units(anl_grid_df, \"degC\")) #> Joining, by = c(\"valid_dttm\", \"units\") #> ::ensemble gridded forecast:: # A tibble: 24 × 8 #>    fcst_dttm           lead_time valid_dttm          units grid_mbr000 #>    <dttm>                  <dbl> <dttm>              <chr>   <geolist> #>  1 2021-01-01 00:00:00         0 2021-01-01 00:00:00 degC      [5 × 5] #>  2 2021-01-01 00:00:00         1 2021-01-01 01:00:00 degC      [5 × 5] #>  3 2021-01-01 00:00:00         2 2021-01-01 02:00:00 degC      [5 × 5] #>  4 2021-01-01 00:00:00         3 2021-01-01 03:00:00 degC      [5 × 5] #>  5 2021-01-01 00:00:00         4 2021-01-01 04:00:00 degC      [5 × 5] #>  6 2021-01-01 00:00:00         5 2021-01-01 05:00:00 degC      [5 × 5] #>  7 2021-01-01 00:00:00         6 2021-01-01 06:00:00 degC      [5 × 5] #>  8 2021-01-01 00:00:00         7 2021-01-01 07:00:00 degC      [5 × 5] #>  9 2021-01-01 00:00:00         8 2021-01-01 08:00:00 degC      [5 × 5] #> 10 2021-01-01 00:00:00         9 2021-01-01 09:00:00 degC      [5 × 5] #> # ℹ 14 more rows #> # ℹ 3 more variables: grid_mbr001 <geolist>, anl_model <chr>, anl <geolist>"},{"path":"/reference/make_verif_groups.html","id":null,"dir":"Reference","previous_headings":"","what":"Combine groups to make a list of all possible verification groups — make_verif_groups","title":"Combine groups to make a list of all possible verification groups — make_verif_groups","text":"function intended define possible group combinations use verification functions det_verify. time_groups given, function ensures appear every group.","code":""},{"path":"/reference/make_verif_groups.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Combine groups to make a list of all possible verification groups — make_verif_groups","text":"","code":"make_verif_groups(time_groups, groups)"},{"path":"/reference/make_verif_groups.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Combine groups to make a list of all possible verification groups — make_verif_groups","text":"time_groups Groups used time axis plot. Can combination \"lead_time\", \"valid_dttm\" \"valid_hour\". groups group columns use verification.","code":""},{"path":"/reference/make_verif_groups.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Combine groups to make a list of all possible verification groups — make_verif_groups","text":"list group combinations","code":""},{"path":"/reference/make_verif_groups.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Combine groups to make a list of all possible verification groups — make_verif_groups","text":"","code":"make_verif_groups(\"lead_time\", c(\"fcst_cycle\", \"station_group\")) #> [[1]] #> [1] \"lead_time\" #>  #> [[2]] #> [1] \"lead_time\"  \"fcst_cycle\" #>  #> [[3]] #> [1] \"lead_time\"     \"station_group\" #>  #> [[4]] #> [1] \"lead_time\"     \"fcst_cycle\"    \"station_group\" #>  make_verif_groups(   c(\"lead_time\", \"valid_dttm\", \"valid_hour\"),   c(\"fcst_cycle\", \"station_group\") ) #> [[1]] #> [1] \"lead_time\" #>  #> [[2]] #> [1] \"valid_dttm\" #>  #> [[3]] #> [1] \"valid_hour\" #>  #> [[4]] #> [1] \"lead_time\"  \"fcst_cycle\" #>  #> [[5]] #> [1] \"lead_time\"     \"station_group\" #>  #> [[6]] #> [1] \"lead_time\"     \"fcst_cycle\"    \"station_group\" #>  #> [[7]] #> [1] \"valid_dttm\" \"fcst_cycle\" #>  #> [[8]] #> [1] \"valid_dttm\"    \"station_group\" #>  #> [[9]] #> [1] \"valid_dttm\"    \"fcst_cycle\"    \"station_group\" #>  #> [[10]] #> [1] \"valid_hour\" \"fcst_cycle\" #>  #> [[11]] #> [1] \"valid_hour\"    \"station_group\" #>  #> [[12]] #> [1] \"valid_hour\"    \"fcst_cycle\"    \"station_group\" #>"},{"path":"/reference/met_functions.html","id":null,"dir":"Reference","previous_headings":"","what":"Compute dewpoint temperature from temperature and relative humidity — met_functions","title":"Compute dewpoint temperature from temperature and relative humidity — met_functions","text":"computation dewpoint temperature done using formula $$\\frac{b\\left(ln(RH) + \\left(\\frac{}{(b+T)}\\right)\\right)}{\\left(-\\left(ln(RH) + \\left(\\frac{}{(b+T)}\\right)\\right)\\right)}$$ computation relative humidity done following $$RH=\\frac{e}{e_s}$$ $$e=\\frac{QP}{0.378Q+0.622}$$ $$e_s=6.112e^{\\frac{}{b+T}}$$ formulae: \\(T\\) temperature \\(\\degree C\\)  \\(RH\\) relative humidity expressed decimal 0 1  \\(Q\\) specific humidity \\(kg.kg^{-1}\\)  \\(P\\) atmospheric pressure \\(hPa\\)  \\(\\) \\(b\\) tuning parameters  default values \\(\\) \\(b\\) 17.67 243.5 respectively recommended NOAA.","code":""},{"path":"/reference/met_functions.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Compute dewpoint temperature from temperature and relative humidity — met_functions","text":"","code":"rh_to_td(rh, t, a = 17.67, b = 243.5)  q_to_rh(q, t, p = 1013.25, a = 17.67, b = 243.5)"},{"path":"/reference/met_functions.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Compute dewpoint temperature from temperature and relative humidity — met_functions","text":"rh vector relative humidities expressed decimal fractions 0 1. maximum greater 5, assumed relative humidities \\(\\%\\) divided 100. t vector length rh temperatures \\(\\degree C\\). maximum greater 200, assumed temperatures \\(Kelvin\\) converted \\(\\degree C\\). tuning parameter. default value \\(17.67\\). b tuning parameter. default value \\(243.5\\). q specific humidity \\(kg.kg^{-1}\\) p air pressure \\(hPa\\)","code":""},{"path":"/reference/met_functions.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Compute dewpoint temperature from temperature and relative humidity — met_functions","text":"numeric vector length t.","code":""},{"path":"/reference/met_functions.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Compute dewpoint temperature from temperature and relative humidity — met_functions","text":"tunings \\(\\) \\(b\\) possible: \\(=17.27\\), \\(b=237.3\\) valid \\(0\\lt T \\lt 60\\) Alternatively: \\(=17.62\\), \\(b=243.12\\) \\(=17.625\\), \\(b=243.04\\) Finally .L. Buck (1981) recommends different tunings based temperature: \\(=17.368\\), \\(b=238.88\\) \\(T \\gt 0\\) \\(=17.966\\), \\(b=247.15\\) \\(T \\lt 0\\) Note formulation used dewpoint temperature take atmospheric pressure account, may correct higher altitudes.","code":""},{"path":"/reference/met_functions.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Compute dewpoint temperature from temperature and relative humidity — met_functions","text":"","code":"rh_to_td(0.8, 20) #> [1] 16.44765  # In Kelvin and % rh_to_td(80, 293.15) #> Warning: rh assumed to be in %. Coverting to 0-1. #> Warning: t assumed to be in K. Converting to degrees C. #> [1] 16.44765 # # Relative humidity is returned as a decimal fraction between 0 and 1 q_to_rh(0.013, 20) #> [1] 0.8990899"},{"path":"/reference/mutate-joins-harp-list.html","id":null,"dir":"Reference","previous_headings":"","what":"Mutating joins for harp_lists — mutate-joins-harp-list","title":"Mutating joins for harp_lists — mutate-joins-harp-list","text":"harp_list methods join functions, example inner_join dplyr package. case x harp_list, y can either data frame another harp_list length x. y data frame, attempt made join y data frame harp_list, x. y harp_list, data frames corresponding elements x y joined. Note done basis location list names taken account. Names output always taken names x.","code":""},{"path":"/reference/mutate-joins-harp-list.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Mutating joins for harp_lists — mutate-joins-harp-list","text":"","code":"# S3 method for harp_list inner_join(x, y, by = NULL, ...)  # S3 method for harp_list left_join(x, y, by = NULL, ...)  # S3 method for harp_list right_join(x, y, by = NULL, ...)  # S3 method for harp_list full_join(x, y, by = NULL, ...)"},{"path":"/reference/mutate-joins-harp-list.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Mutating joins for harp_lists — mutate-joins-harp-list","text":"x, y pair data frames, data frame extensions (e.g. tibble), lazy data frames (e.g. dbplyr dtplyr). See Methods, , details. join specification created join_by(), character vector variables join . NULL, default, *_join() perform natural join, using variables common across x y. message lists variables can check correct; suppress message supplying explicitly. join different variables x y, use join_by() specification. example, join_by(== b) match x$y$b. join multiple variables, use join_by() specification multiple expressions. example, join_by(== b, c == d) match x$y$b x$c y$d. column names x y, can shorten listing variable names, like join_by(, c). join_by() can also used perform inequality, rolling, overlap joins. See documentation ?join_by details types joins. simple equality joins, can alternatively specify character vector variable names join . example, = c(\"\", \"b\") joins x$y$x$b y$b. variable names differ x y, use named character vector like = c(\"x_a\" = \"y_a\", \"x_b\" = \"y_b\"). perform cross-join, generating combinations x y, see cross_join(). ... Arguments passed dplyr::inner_join, dplyr::left_join, dplyr::right_join, dplyr::full_join suffix non-joined duplicate variables x y, suffixes added output disambiguate . character vector length 2. keep join keys x y preserved output? NULL, default, joins equality retain keys x, joins inequality retain keys inputs. TRUE, keys inputs retained. FALSE, keys x retained. right full joins, data key columns corresponding rows exist y merged key columns x. used joining inequality conditions.","code":""},{"path":"/reference/mutate-joins-harp-list.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Mutating joins for harp_lists — mutate-joins-harp-list","text":"harp_list names x.","code":""},{"path":"/reference/mutate.harp_list.html","id":null,"dir":"Reference","previous_headings":"","what":"Create, modify and delete columns — mutate.harp_list","title":"Create, modify and delete columns — mutate.harp_list","text":"harp_list method mutate(). works exactly way except harp_list returned. use function, dplyr package must attached .harp_list suffix can dropped. Note transmute() mutate(), except modified / created columns kept.","code":""},{"path":"/reference/mutate.harp_list.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Create, modify and delete columns — mutate.harp_list","text":"","code":"# S3 method for harp_list mutate(.data, ...)  # S3 method for harp_list transmute(.data, ...)"},{"path":"/reference/mutate.harp_list.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Create, modify and delete columns — mutate.harp_list","text":".data data frame, data frame extension (e.g. tibble), lazy data frame (e.g. dbplyr dtplyr). See Methods, , details. ... <data-masking> Name-value pairs. name gives name column output. value can : vector length 1, recycled correct length. vector length current group (whole data frame ungrouped). NULL, remove column. data frame tibble, create multiple columns output.","code":""},{"path":"/reference/nbhd_smooth.html","id":null,"dir":"Reference","previous_headings":"","what":"Smooth a 2d field based an a neighbourhood radius — nbhd_smooth","title":"Smooth a 2d field based an a neighbourhood radius — nbhd_smooth","text":"nbhd_smooth takes 2d field, pixel field square neighbourhood constructed specified radius - number pixels left/right /pixel neighbourhood centre. pixel, mean value pixels neighbourhood returned resulting smoothed field grid original field.","code":""},{"path":"/reference/nbhd_smooth.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Smooth a 2d field based an a neighbourhood radius — nbhd_smooth","text":"","code":"nbhd_smooth(   x,   radius,   threshold = NA,   comparator = c(\"ge\", \"gt\", \"le\", \"lt\", \"between\", \"outside\"),   include_low = TRUE,   include_high = TRUE,   boundary = c(\"zero_pad\", \"missing\") )  cumsum_2d(   x,   threshold = NA,   comparator = c(\"ge\", \"gt\", \"le\", \"lt\", \"between\", \"outside\"),   include_low = TRUE,   include_high = TRUE )  nbhd_smooth_cumsum(x, radius, boundary = c(\"zero_pad\", \"missing\"))"},{"path":"/reference/nbhd_smooth.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Smooth a 2d field based an a neighbourhood radius — nbhd_smooth","text":"x 2d array, geofield geolist. radius radius neighbourhood pixels. threshold threshold computing binary probabilities. comparator = \"\", must two element vector. Set NA use raw data. comparator compare x threshold compute binary probabilities. Can \"ge\", \"gt\", \"le\", \"lt\" >=, >, <= < respectively. Can also \"\" \"outside\", case binary probability outside two values given threshold computed. include_low Logical. Whether include lower two thresholds comparison comparator = \"\" comparator = \"outside\". include_high Logical. Whether include higher two thresholds comparison comparator = \"\" comparator = \"outside\". boundary treat boundaries. Current options \"zero_pad\" \"missing\". See Details information options.","code":""},{"path":"/reference/nbhd_smooth.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Smooth a 2d field based an a neighbourhood radius — nbhd_smooth","text":"smoothed 2d field. case geofields geolists, attributes retained.","code":""},{"path":"/reference/nbhd_smooth.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Smooth a 2d field based an a neighbourhood radius — nbhd_smooth","text":"pixels closer edge input field radius two options. default zero pad field outwards pixels outside original field set zero. boundary = \"missing\" pixels closer edge radius set missing. neighbourhood means computed first calculating cumulative sum array top left bottom right computing total neighbourhood subtraction. means algorithm equally efficient regardless neighbourhood size. possible compute stage time fist cumsum_2d nbhd_smooth_cumsum. threshold given, binary probability input field compared threshold computed pixel applying function. default comparison x >= threshold, comparisons can chosen using comparator argument.","code":""},{"path":"/reference/nbhd_smooth.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Smooth a 2d field based an a neighbourhood radius — nbhd_smooth","text":"","code":"# Create a 2d array z <- array(dim = c(100, 100)) for (i in 1:100) {   for (j in 1:100) {     z[i, j] <- sin(i / 10) + sin(j / 10)  } }  image(z, zlim = range(z), col = heat.colors(255))  # With zero-padding the input array image(nbhd_smooth(z, 5), zlim = range(z), col = heat.colors(255))  image(nbhd_smooth(z, 10), zlim = range(z), col = heat.colors(255))  image(nbhd_smooth(z, 20), zlim = range(z), col = heat.colors(255))   # Without zero padding image(nbhd_smooth(z, 5, boundary = \"missing\"), zlim = range(z), col = heat.colors(255))  image(nbhd_smooth(z, 10, boundary = \"missing\"), zlim = range(z), col = heat.colors(255))   # Add a threshold image(nbhd_smooth(z, 10, 0.5), zlim = c(0, 1), col = heat.colors(255))  image(nbhd_smooth(z, 10, 0.5, \"lt\"), zlim = c(0, 1), col = heat.colors(255))  image(nbhd_smooth(z, 10, c(-0.5, 0.5), \"between\"), zlim = c(0, 1), col = heat.colors(255))  image(nbhd_smooth(z, 10, c(-0.5, 0.5), \"outside\"), zlim = c(0, 1), col = heat.colors(255))"},{"path":"/reference/parse_thresholds.html","id":null,"dir":"Reference","previous_headings":"","what":"Decide whether thresholds are absolute or quantiles — parse_thresholds","title":"Decide whether thresholds are absolute or quantiles — parse_thresholds","text":"quantiles, thresholds expressed character vector element beginning \"q\" followed quantile expressed value 0 1.","code":""},{"path":"/reference/parse_thresholds.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Decide whether thresholds are absolute or quantiles — parse_thresholds","text":"","code":"parse_thresholds(x)"},{"path":"/reference/parse_thresholds.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Decide whether thresholds are absolute or quantiles — parse_thresholds","text":"x numeric character vector.","code":""},{"path":"/reference/parse_thresholds.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Decide whether thresholds are absolute or quantiles — parse_thresholds","text":"list numeric vector thresholds logical indicating whether thresholds quantiles.","code":""},{"path":"/reference/parse_thresholds.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Decide whether thresholds are absolute or quantiles — parse_thresholds","text":"","code":"parse_thresholds(seq(1, 5)) #> $thresholds #> [1] 1 2 3 4 5 #>  #> $quantiles #> [1] FALSE #>  parse_thresholds(paste0(\"q\", seq(0, 1, 0.2))) #> $thresholds #> [1] 0.0 0.2 0.4 0.6 0.8 1.0 #>  #> $quantiles #> [1] TRUE #>"},{"path":"/reference/pipe.html","id":null,"dir":"Reference","previous_headings":"","what":"Pipe operator — %>%","title":"Pipe operator — %>%","text":"See magrittr::%>% details.","code":""},{"path":"/reference/pipe.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Pipe operator — %>%","text":"","code":"lhs %>% rhs"},{"path":"/reference/pivot_members.html","id":null,"dir":"Reference","previous_headings":"","what":"Pivot between wide and long ensemble data frames — pivot_members","title":"Pivot between wide and long ensemble data frames — pivot_members","text":"default behaviour harp store ensemble data wide data frames. means one column member ensemble. always ideal goes principles tidy data, whereby ensemble member stored separate row single column denoting ensemble member. pivot_members can used pivot wide long formats directions.","code":""},{"path":"/reference/pivot_members.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Pivot between wide and long ensemble data frames — pivot_members","text":"","code":"pivot_members(.data)"},{"path":"/reference/pivot_members.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Pivot between wide and long ensemble data frames — pivot_members","text":".data harp data frame harp_list data frames","code":""},{"path":"/reference/pivot_members.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Pivot between wide and long ensemble data frames — pivot_members","text":"data frame, harp_list members pivoted.","code":""},{"path":"/reference/pivot_members.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Pivot between wide and long ensemble data frames — pivot_members","text":"pivoting wide long data frame, class updated indicate ensemble members stored rows rather columns. pivoting back wide data frame format, class returned original names.","code":""},{"path":"/reference/pivot_members.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Pivot between wide and long ensemble data frames — pivot_members","text":"","code":"pivot_members(ens_point_df) #> ::ensemble point forecast [[long]]:: # A tibble: 96 × 7 #>    sub_model fcst_dttm           lead_time valid_dttm            SID member #>  * <chr>     <dttm>                  <dbl> <dttm>              <dbl> <chr>  #>  1 point     2021-01-01 00:00:00         0 2021-01-01 00:00:00  1001 mbr000 #>  2 point     2021-01-01 00:00:00         0 2021-01-01 00:00:00  1001 mbr001 #>  3 point     2021-01-01 00:00:00         1 2021-01-01 01:00:00  1001 mbr000 #>  4 point     2021-01-01 00:00:00         1 2021-01-01 01:00:00  1001 mbr001 #>  5 point     2021-01-01 00:00:00         2 2021-01-01 02:00:00  1001 mbr000 #>  6 point     2021-01-01 00:00:00         2 2021-01-01 02:00:00  1001 mbr001 #>  7 point     2021-01-01 00:00:00         3 2021-01-01 03:00:00  1001 mbr000 #>  8 point     2021-01-01 00:00:00         3 2021-01-01 03:00:00  1001 mbr001 #>  9 point     2021-01-01 00:00:00         4 2021-01-01 04:00:00  1001 mbr000 #> 10 point     2021-01-01 00:00:00         4 2021-01-01 04:00:00  1001 mbr001 #> # ℹ 86 more rows #> # ℹ 1 more variable: fcst <dbl> pivot_members(ens_grid_df) #> ::ensemble gridded forecast [[long]]:: # A tibble: 48 × 6 #>    sub_model fcst_dttm           lead_time valid_dttm          member      fcst #>  * <chr>     <dttm>                  <dbl> <dttm>              <chr>  <geolist> #>  1 grid      2021-01-01 00:00:00         0 2021-01-01 00:00:00 mbr000   [5 × 5] #>  2 grid      2021-01-01 00:00:00         0 2021-01-01 00:00:00 mbr001   [5 × 5] #>  3 grid      2021-01-01 00:00:00         1 2021-01-01 01:00:00 mbr000   [5 × 5] #>  4 grid      2021-01-01 00:00:00         1 2021-01-01 01:00:00 mbr001   [5 × 5] #>  5 grid      2021-01-01 00:00:00         2 2021-01-01 02:00:00 mbr000   [5 × 5] #>  6 grid      2021-01-01 00:00:00         2 2021-01-01 02:00:00 mbr001   [5 × 5] #>  7 grid      2021-01-01 00:00:00         3 2021-01-01 03:00:00 mbr000   [5 × 5] #>  8 grid      2021-01-01 00:00:00         3 2021-01-01 03:00:00 mbr001   [5 × 5] #>  9 grid      2021-01-01 00:00:00         4 2021-01-01 04:00:00 mbr000   [5 × 5] #> 10 grid      2021-01-01 00:00:00         4 2021-01-01 04:00:00 mbr001   [5 × 5] #> # ℹ 38 more rows pivot_members(ens_point_list) #> • a #> ::ensemble point forecast [[long]]:: # A tibble: 96 × 8 #>    fcst_model sub_model fcst_dttm           lead_time valid_dttm            SID #>  * <chr>      <chr>     <dttm>                  <dbl> <dttm>              <dbl> #>  1 a          a         2021-01-01 00:00:00         0 2021-01-01 00:00:00  1001 #>  2 a          a         2021-01-01 00:00:00         0 2021-01-01 00:00:00  1001 #>  3 a          a         2021-01-01 00:00:00         1 2021-01-01 01:00:00  1001 #>  4 a          a         2021-01-01 00:00:00         1 2021-01-01 01:00:00  1001 #>  5 a          a         2021-01-01 00:00:00         2 2021-01-01 02:00:00  1001 #>  6 a          a         2021-01-01 00:00:00         2 2021-01-01 02:00:00  1001 #>  7 a          a         2021-01-01 00:00:00         3 2021-01-01 03:00:00  1001 #>  8 a          a         2021-01-01 00:00:00         3 2021-01-01 03:00:00  1001 #>  9 a          a         2021-01-01 00:00:00         4 2021-01-01 04:00:00  1001 #> 10 a          a         2021-01-01 00:00:00         4 2021-01-01 04:00:00  1001 #> # ℹ 86 more rows #> # ℹ 2 more variables: member <chr>, fcst <dbl> #>  #> • b #> ::ensemble point forecast [[long]]:: # A tibble: 96 × 8 #>    fcst_model sub_model fcst_dttm           lead_time valid_dttm            SID #>  * <chr>      <chr>     <dttm>                  <dbl> <dttm>              <dbl> #>  1 b          b         2021-01-01 00:00:00         0 2021-01-01 00:00:00  1001 #>  2 b          b         2021-01-01 00:00:00         0 2021-01-01 00:00:00  1001 #>  3 b          b         2021-01-01 00:00:00         1 2021-01-01 01:00:00  1001 #>  4 b          b         2021-01-01 00:00:00         1 2021-01-01 01:00:00  1001 #>  5 b          b         2021-01-01 00:00:00         2 2021-01-01 02:00:00  1001 #>  6 b          b         2021-01-01 00:00:00         2 2021-01-01 02:00:00  1001 #>  7 b          b         2021-01-01 00:00:00         3 2021-01-01 03:00:00  1001 #>  8 b          b         2021-01-01 00:00:00         3 2021-01-01 03:00:00  1001 #>  9 b          b         2021-01-01 00:00:00         4 2021-01-01 04:00:00  1001 #> 10 b          b         2021-01-01 00:00:00         4 2021-01-01 04:00:00  1001 #> # ℹ 86 more rows #> # ℹ 2 more variables: member <chr>, fcst <dbl> #>  pivot_members(ens_grid_list) #> • a #> ::ensemble gridded forecast [[long]]:: # A tibble: 48 × 7 #>    fcst_model sub_model fcst_dttm           lead_time valid_dttm          member #>  * <chr>      <chr>     <dttm>                  <dbl> <dttm>              <chr>  #>  1 a          a         2021-01-01 00:00:00         0 2021-01-01 00:00:00 mbr000 #>  2 a          a         2021-01-01 00:00:00         0 2021-01-01 00:00:00 mbr001 #>  3 a          a         2021-01-01 00:00:00         1 2021-01-01 01:00:00 mbr000 #>  4 a          a         2021-01-01 00:00:00         1 2021-01-01 01:00:00 mbr001 #>  5 a          a         2021-01-01 00:00:00         2 2021-01-01 02:00:00 mbr000 #>  6 a          a         2021-01-01 00:00:00         2 2021-01-01 02:00:00 mbr001 #>  7 a          a         2021-01-01 00:00:00         3 2021-01-01 03:00:00 mbr000 #>  8 a          a         2021-01-01 00:00:00         3 2021-01-01 03:00:00 mbr001 #>  9 a          a         2021-01-01 00:00:00         4 2021-01-01 04:00:00 mbr000 #> 10 a          a         2021-01-01 00:00:00         4 2021-01-01 04:00:00 mbr001 #> # ℹ 38 more rows #> # ℹ 1 more variable: fcst <geolist> #>  #> • b #> ::ensemble gridded forecast [[long]]:: # A tibble: 48 × 7 #>    fcst_model sub_model fcst_dttm           lead_time valid_dttm          member #>  * <chr>      <chr>     <dttm>                  <dbl> <dttm>              <chr>  #>  1 b          b         2021-01-01 00:00:00         0 2021-01-01 00:00:00 mbr000 #>  2 b          b         2021-01-01 00:00:00         0 2021-01-01 00:00:00 mbr001 #>  3 b          b         2021-01-01 00:00:00         1 2021-01-01 01:00:00 mbr000 #>  4 b          b         2021-01-01 00:00:00         1 2021-01-01 01:00:00 mbr001 #>  5 b          b         2021-01-01 00:00:00         2 2021-01-01 02:00:00 mbr000 #>  6 b          b         2021-01-01 00:00:00         2 2021-01-01 02:00:00 mbr001 #>  7 b          b         2021-01-01 00:00:00         3 2021-01-01 03:00:00 mbr000 #>  8 b          b         2021-01-01 00:00:00         3 2021-01-01 03:00:00 mbr001 #>  9 b          b         2021-01-01 00:00:00         4 2021-01-01 04:00:00 mbr000 #> 10 b          b         2021-01-01 00:00:00         4 2021-01-01 04:00:00 mbr001 #> # ℹ 38 more rows #> # ℹ 1 more variable: fcst <geolist> #>   # Note the change in class class(ens_point_df) #> [1] \"harp_ens_point_df\" \"harp_point_df\"     \"harp_df\"           #> [4] \"tbl_df\"            \"tbl\"               \"data.frame\"        class(pivot_members(ens_point_df)) #> [1] \"harp_ens_point_df_long\" \"harp_point_df\"          \"harp_df\"                #> [4] \"tbl_df\"                 \"tbl\"                    \"data.frame\"             class(ens_grid_df) #> [1] \"harp_ens_grid_df\" \"harp_grid_df\"     \"harp_df\"          \"tbl_df\"           #> [5] \"tbl\"              \"data.frame\"       class(pivot_members(ens_grid_df)) #> [1] \"harp_ens_grid_df_long\" \"harp_grid_df\"          \"harp_df\"               #> [4] \"tbl_df\"                \"tbl\"                   \"data.frame\""},{"path":"/reference/psub.html","id":null,"dir":"Reference","previous_headings":"","what":"Piecewise pattern matching and replacement — psub","title":"Piecewise pattern matching and replacement — psub","text":"psub calls gsub pattern / replacement pair. pattern found, warning issued nothing replaced pattern / replacement pair. Unlike gsub, first argument vector matches sought, meaning can easily used pipeline.","code":""},{"path":"/reference/psub.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Piecewise pattern matching and replacement — psub","text":"","code":"psub(x, pattern, replacement, exact = TRUE, ...)"},{"path":"/reference/psub.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Piecewise pattern matching and replacement — psub","text":"x character vector. pattern vector patterns replace. replacement character vector length pattern corresponding replacements. exact logical denote whether pattern exact match (default). FALSE, pattern treated regular expression. ... arguments gsub.","code":""},{"path":"/reference/psub.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Piecewise pattern matching and replacement — psub","text":"character vector length x matched patterns replaced.","code":""},{"path":"/reference/psub.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Piecewise pattern matching and replacement — psub","text":"default behaviour look exact match pattern. means elements x fully match pattern replaced. See examples works practice.","code":""},{"path":"/reference/psub.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Piecewise pattern matching and replacement — psub","text":"","code":"# Capitalise some letters psub(letters[1:7], c(\"a\", \"c\", \"e\"), c(\"A\", \"C\", \"E\")) #> [1] \"A\" \"b\" \"C\" \"d\" \"E\" \"f\" \"g\"  # By default exact matches are sought psub(\"ace\", c(\"a\", \"c\", \"e\"), c(\"A\", \"C\", \"E\")) #> Warning: \"a\" not found in `x`. #> Warning: \"c\" not found in `x`. #> Warning: \"e\" not found in `x`. #> [1] \"ace\"  # Use exact = FALSE to replace a regular expression psub(\"ace\", c(\"a\", \"c\", \"e\"), c(\"A\", \"C\", \"E\"), exact = FALSE) #> [1] \"ACE\"  # Replace selected values in a vector psub(c(\"one\", \"two\", \"three\"), c(\"one\", \"three\"), c(\"ONE\", \"THREE\")) #> [1] \"ONE\"   \"two\"   \"THREE\""},{"path":"/reference/pull.harp_list.html","id":null,"dir":"Reference","previous_headings":"","what":"Extract a single column — pull.harp_list","title":"Extract a single column — pull.harp_list","text":"harp_list method pull(). works exactly way except list vectors returned. use function, dplyr package must attached .harp_list suffix can dropped.","code":""},{"path":"/reference/pull.harp_list.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Extract a single column — pull.harp_list","text":"","code":"# S3 method for harp_list pull(.data, ...)"},{"path":"/reference/pull.harp_list.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Extract a single column — pull.harp_list","text":".data data frame, data frame extension (e.g. tibble), lazy data frame (e.g. dbplyr dtplyr). See Methods, , details. ... use methods.","code":""},{"path":"/reference/reexports.html","id":null,"dir":"Reference","previous_headings":"","what":"Objects exported from other packages — reexports","title":"Objects exported from other packages — reexports","text":"objects imported packages. Follow links see documentation. dplyr filter","code":""},{"path":"/reference/rename.harp_list.html","id":null,"dir":"Reference","previous_headings":"","what":"Rename columns — rename.harp_list","title":"Rename columns — rename.harp_list","text":"harp_list method rename(). works exactly way except harp_list returned. use function, dplyr package must attached .harp_list suffix can dropped.","code":""},{"path":"/reference/rename.harp_list.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Rename columns — rename.harp_list","text":"","code":"# S3 method for harp_list rename(.data, ...)  # S3 method for harp_list rename_with(.data, .fn, ...)"},{"path":"/reference/rename.harp_list.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Rename columns — rename.harp_list","text":".data data frame, data frame extension (e.g. tibble), lazy data frame (e.g. dbplyr dtplyr). See Methods, , details. ... rename(): <tidy-select> Use new_name = old_name rename selected variables. rename_with(): additional arguments passed onto .fn. .fn function used transform selected .cols. return character vector length input.","code":""},{"path":"/reference/rename.harp_list.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Rename columns — rename.harp_list","text":"rename_with, .fn argument mandatory following rename, .cols argument selects columns rename default.","code":""},{"path":"/reference/scale_param.html","id":null,"dir":"Reference","previous_headings":"","what":"Scale a parameter in a data frame — scale_param","title":"Scale a parameter in a data frame — scale_param","text":"Scale parameter data frame","code":""},{"path":"/reference/scale_param.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Scale a parameter in a data frame — scale_param","text":"","code":"scale_param(x, scaling, new_units, mult = FALSE, ...)  # S3 method for data.frame scale_param(x, scaling, new_units, mult = FALSE, col, ...)"},{"path":"/reference/scale_param.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Scale a parameter in a data frame — scale_param","text":"x data frame harp_list. scaling scaling apply data. default scaling additive, mult = TRUE multiplicative. new_units name new units. missing, units name unchanged. mult Logical. Whether scaling multiplicative. default FALSE, meaning scaling additive. ... Used methods. col name column scale - x forecast analysis harp_df data frame harp_list, columns selected automatically col ignored.","code":""},{"path":"/reference/scale_param.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Scale a parameter in a data frame — scale_param","text":"data frame harp_list scaled parameter.","code":""},{"path":"/reference/scale_param.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Scale a parameter in a data frame — scale_param","text":"","code":"# Make a data frame of 2m temperature observations in degrees C library(tibble) obs <- tibble(   valid_date = rep(seq_dttm(2022081500, 2022081523), 3),   SID        = c(rep(1001, 24), rep(1002, 24), rep(1003, 24)),   units      = \"degC\",   T2m        = rnorm(24 * 3, 15, 2) )  # Scale to be in Kelvin scale_param(obs, 273.15, \"K\", col = T2m) #> # A tibble: 72 × 4 #>    valid_date   SID units   T2m #>    <chr>      <dbl> <chr> <dbl> #>  1 2022081500  1001 K      289. #>  2 2022081501  1001 K      286. #>  3 2022081502  1001 K      287. #>  4 2022081503  1001 K      285. #>  5 2022081504  1001 K      289. #>  6 2022081505  1001 K      288. #>  7 2022081506  1001 K      289. #>  8 2022081507  1001 K      291. #>  9 2022081508  1001 K      286. #> 10 2022081509  1001 K      289. #> # ℹ 62 more rows  # col can be a quoted, or if a variable is must be wrapped in {{}} scale_param(obs, 273.15, \"K\", col = \"T2m\") #> # A tibble: 72 × 4 #>    valid_date   SID units   T2m #>    <chr>      <dbl> <chr> <dbl> #>  1 2022081500  1001 K      289. #>  2 2022081501  1001 K      286. #>  3 2022081502  1001 K      287. #>  4 2022081503  1001 K      285. #>  5 2022081504  1001 K      289. #>  6 2022081505  1001 K      288. #>  7 2022081506  1001 K      289. #>  8 2022081507  1001 K      291. #>  9 2022081508  1001 K      286. #> 10 2022081509  1001 K      289. #> # ℹ 62 more rows prm <- \"T2m\" scale_param(obs, 273.15, \"K\", col = {{prm}}) #> # A tibble: 72 × 4 #>    valid_date   SID units   T2m #>    <chr>      <dbl> <chr> <dbl> #>  1 2022081500  1001 K      289. #>  2 2022081501  1001 K      286. #>  3 2022081502  1001 K      287. #>  4 2022081503  1001 K      285. #>  5 2022081504  1001 K      289. #>  6 2022081505  1001 K      288. #>  7 2022081506  1001 K      289. #>  8 2022081507  1001 K      291. #>  9 2022081508  1001 K      286. #> 10 2022081509  1001 K      289. #> # ℹ 62 more rows  # For forecast data frames, col is not needed scale_param(det_point_df, 273.15, \"K\") #> ::deterministic point forecast:: # A tibble: 48 × 7 #>    fcst_model fcst_dttm           lead_time valid_dttm            SID units #>    <chr>      <dttm>                  <dbl> <dttm>              <dbl> <chr> #>  1 point      2021-01-01 00:00:00         0 2021-01-01 00:00:00  1001 K     #>  2 point      2021-01-01 00:00:00         1 2021-01-01 01:00:00  1001 K     #>  3 point      2021-01-01 00:00:00         2 2021-01-01 02:00:00  1001 K     #>  4 point      2021-01-01 00:00:00         3 2021-01-01 03:00:00  1001 K     #>  5 point      2021-01-01 00:00:00         4 2021-01-01 04:00:00  1001 K     #>  6 point      2021-01-01 00:00:00         5 2021-01-01 05:00:00  1001 K     #>  7 point      2021-01-01 00:00:00         6 2021-01-01 06:00:00  1001 K     #>  8 point      2021-01-01 00:00:00         7 2021-01-01 07:00:00  1001 K     #>  9 point      2021-01-01 00:00:00         8 2021-01-01 08:00:00  1001 K     #> 10 point      2021-01-01 00:00:00         9 2021-01-01 09:00:00  1001 K     #> # ℹ 38 more rows #> # ℹ 1 more variable: fcst <dbl> scale_param(ens_point_df, 273.15, \"K\") #> ::ensemble point forecast:: # A tibble: 48 × 7 #>    fcst_dttm           lead_time valid_dttm            SID units point_mbr000 #>    <dttm>                  <dbl> <dttm>              <dbl> <chr>        <dbl> #>  1 2021-01-01 00:00:00         0 2021-01-01 00:00:00  1001 K             273. #>  2 2021-01-01 00:00:00         1 2021-01-01 01:00:00  1001 K             274. #>  3 2021-01-01 00:00:00         2 2021-01-01 02:00:00  1001 K             274. #>  4 2021-01-01 00:00:00         3 2021-01-01 03:00:00  1001 K             274. #>  5 2021-01-01 00:00:00         4 2021-01-01 04:00:00  1001 K             273. #>  6 2021-01-01 00:00:00         5 2021-01-01 05:00:00  1001 K             274. #>  7 2021-01-01 00:00:00         6 2021-01-01 06:00:00  1001 K             273. #>  8 2021-01-01 00:00:00         7 2021-01-01 07:00:00  1001 K             274. #>  9 2021-01-01 00:00:00         8 2021-01-01 08:00:00  1001 K             274. #> 10 2021-01-01 00:00:00         9 2021-01-01 09:00:00  1001 K             274. #> # ℹ 38 more rows #> # ℹ 1 more variable: point_mbr001 <dbl>  # Scaling can be multiplicative scale_param(det_point_df, 100, \"percent\", mult = TRUE) #> ::deterministic point forecast:: # A tibble: 48 × 7 #>    fcst_model fcst_dttm           lead_time valid_dttm            SID units   #>    <chr>      <dttm>                  <dbl> <dttm>              <dbl> <chr>   #>  1 point      2021-01-01 00:00:00         0 2021-01-01 00:00:00  1001 percent #>  2 point      2021-01-01 00:00:00         1 2021-01-01 01:00:00  1001 percent #>  3 point      2021-01-01 00:00:00         2 2021-01-01 02:00:00  1001 percent #>  4 point      2021-01-01 00:00:00         3 2021-01-01 03:00:00  1001 percent #>  5 point      2021-01-01 00:00:00         4 2021-01-01 04:00:00  1001 percent #>  6 point      2021-01-01 00:00:00         5 2021-01-01 05:00:00  1001 percent #>  7 point      2021-01-01 00:00:00         6 2021-01-01 06:00:00  1001 percent #>  8 point      2021-01-01 00:00:00         7 2021-01-01 07:00:00  1001 percent #>  9 point      2021-01-01 00:00:00         8 2021-01-01 08:00:00  1001 percent #> 10 point      2021-01-01 00:00:00         9 2021-01-01 09:00:00  1001 percent #> # ℹ 38 more rows #> # ℹ 1 more variable: fcst <dbl> scale_param(ens_point_list, 1/1000, \"kg/kg\", mult = TRUE) #> • a #> ::ensemble point forecast:: # A tibble: 48 × 8 #>    fcst_model fcst_dttm           lead_time valid_dttm            SID units #>    <chr>      <dttm>                  <dbl> <dttm>              <dbl> <chr> #>  1 a          2021-01-01 00:00:00         0 2021-01-01 00:00:00  1001 kg/kg #>  2 a          2021-01-01 00:00:00         1 2021-01-01 01:00:00  1001 kg/kg #>  3 a          2021-01-01 00:00:00         2 2021-01-01 02:00:00  1001 kg/kg #>  4 a          2021-01-01 00:00:00         3 2021-01-01 03:00:00  1001 kg/kg #>  5 a          2021-01-01 00:00:00         4 2021-01-01 04:00:00  1001 kg/kg #>  6 a          2021-01-01 00:00:00         5 2021-01-01 05:00:00  1001 kg/kg #>  7 a          2021-01-01 00:00:00         6 2021-01-01 06:00:00  1001 kg/kg #>  8 a          2021-01-01 00:00:00         7 2021-01-01 07:00:00  1001 kg/kg #>  9 a          2021-01-01 00:00:00         8 2021-01-01 08:00:00  1001 kg/kg #> 10 a          2021-01-01 00:00:00         9 2021-01-01 09:00:00  1001 kg/kg #> # ℹ 38 more rows #> # ℹ 2 more variables: a_mbr000 <dbl>, a_mbr001 <dbl> #>  #> • b #> ::ensemble point forecast:: # A tibble: 48 × 8 #>    fcst_model fcst_dttm           lead_time valid_dttm            SID units #>    <chr>      <dttm>                  <dbl> <dttm>              <dbl> <chr> #>  1 b          2021-01-01 00:00:00         0 2021-01-01 00:00:00  1001 kg/kg #>  2 b          2021-01-01 00:00:00         1 2021-01-01 01:00:00  1001 kg/kg #>  3 b          2021-01-01 00:00:00         2 2021-01-01 02:00:00  1001 kg/kg #>  4 b          2021-01-01 00:00:00         3 2021-01-01 03:00:00  1001 kg/kg #>  5 b          2021-01-01 00:00:00         4 2021-01-01 04:00:00  1001 kg/kg #>  6 b          2021-01-01 00:00:00         5 2021-01-01 05:00:00  1001 kg/kg #>  7 b          2021-01-01 00:00:00         6 2021-01-01 06:00:00  1001 kg/kg #>  8 b          2021-01-01 00:00:00         7 2021-01-01 07:00:00  1001 kg/kg #>  9 b          2021-01-01 00:00:00         8 2021-01-01 08:00:00  1001 kg/kg #> 10 b          2021-01-01 00:00:00         9 2021-01-01 09:00:00  1001 kg/kg #> # ℹ 38 more rows #> # ℹ 2 more variables: b_mbr000 <dbl>, b_mbr001 <dbl> #>"},{"path":"/reference/select.harp_list.html","id":null,"dir":"Reference","previous_headings":"","what":"Subset columns using their names and types — select.harp_list","title":"Subset columns using their names and types — select.harp_list","text":"harp_list method select(). works exactly way except harp_list returned. use function, dplyr package must attached .harp_list suffix can dropped.","code":""},{"path":"/reference/select.harp_list.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Subset columns using their names and types — select.harp_list","text":"","code":"# S3 method for harp_list select(.data, ...)"},{"path":"/reference/select.harp_list.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Subset columns using their names and types — select.harp_list","text":".data data frame, data frame extension (e.g. tibble), lazy data frame (e.g. dbplyr dtplyr). See Methods, , details. ... <tidy-select> One unquoted expressions separated commas. Variable names can used positions data frame, expressions like x:y can used select range variables.","code":""},{"path":"/reference/select_members.html","id":null,"dir":"Reference","previous_headings":"","what":"Select ensemble members — select_members","title":"Select ensemble members — select_members","text":"select_members used select specific ensemble members data frame along columns. method can also applied harp_list ensemble data frames.","code":""},{"path":"/reference/select_members.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Select ensemble members — select_members","text":"","code":"select_members(.data, members, include_lagged = TRUE, invert = FALSE)"},{"path":"/reference/select_members.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Select ensemble members — select_members","text":".data data frame ensemble members harp_list members members select. Can numeric vector, named list select members specific forecast models harp_list object. include_lagged Logical. Whether include lagged ensemble members selection. invert Logical. TRUE members except provided members returned.","code":""},{"path":"/reference/select_members.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Select ensemble members — select_members","text":"data frame selected members harp_list data frames selected ensemble members.","code":""},{"path":"/reference/select_members.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Select ensemble members — select_members","text":"","code":"select_members(ens_point_df, 0) #> ::ensemble point forecast:: # A tibble: 48 × 5 #>    fcst_dttm           lead_time valid_dttm            SID point_mbr000 #>    <dttm>                  <dbl> <dttm>              <dbl>        <dbl> #>  1 2021-01-01 00:00:00         0 2021-01-01 00:00:00  1001       0.277  #>  2 2021-01-01 00:00:00         1 2021-01-01 01:00:00  1001       0.650  #>  3 2021-01-01 00:00:00         2 2021-01-01 02:00:00  1001       0.601  #>  4 2021-01-01 00:00:00         3 2021-01-01 03:00:00  1001       0.427  #>  5 2021-01-01 00:00:00         4 2021-01-01 04:00:00  1001       0.0798 #>  6 2021-01-01 00:00:00         5 2021-01-01 05:00:00  1001       0.762  #>  7 2021-01-01 00:00:00         6 2021-01-01 06:00:00  1001       0.347  #>  8 2021-01-01 00:00:00         7 2021-01-01 07:00:00  1001       0.488  #>  9 2021-01-01 00:00:00         8 2021-01-01 08:00:00  1001       0.438  #> 10 2021-01-01 00:00:00         9 2021-01-01 09:00:00  1001       0.662  #> # ℹ 38 more rows select_members(ens_grid_df, 1) #> ::ensemble gridded forecast:: # A tibble: 24 × 4 #>    fcst_dttm           lead_time valid_dttm          grid_mbr001 #>    <dttm>                  <dbl> <dttm>                <geolist> #>  1 2021-01-01 00:00:00         0 2021-01-01 00:00:00     [5 × 5] #>  2 2021-01-01 00:00:00         1 2021-01-01 01:00:00     [5 × 5] #>  3 2021-01-01 00:00:00         2 2021-01-01 02:00:00     [5 × 5] #>  4 2021-01-01 00:00:00         3 2021-01-01 03:00:00     [5 × 5] #>  5 2021-01-01 00:00:00         4 2021-01-01 04:00:00     [5 × 5] #>  6 2021-01-01 00:00:00         5 2021-01-01 05:00:00     [5 × 5] #>  7 2021-01-01 00:00:00         6 2021-01-01 06:00:00     [5 × 5] #>  8 2021-01-01 00:00:00         7 2021-01-01 07:00:00     [5 × 5] #>  9 2021-01-01 00:00:00         8 2021-01-01 08:00:00     [5 × 5] #> 10 2021-01-01 00:00:00         9 2021-01-01 09:00:00     [5 × 5] #> # ℹ 14 more rows  # More than one member can be selected select_members(ens_point_df, c(0, 1)) #> ::ensemble point forecast:: # A tibble: 48 × 6 #>    fcst_dttm           lead_time valid_dttm            SID point_mbr000 #>    <dttm>                  <dbl> <dttm>              <dbl>        <dbl> #>  1 2021-01-01 00:00:00         0 2021-01-01 00:00:00  1001       0.277  #>  2 2021-01-01 00:00:00         1 2021-01-01 01:00:00  1001       0.650  #>  3 2021-01-01 00:00:00         2 2021-01-01 02:00:00  1001       0.601  #>  4 2021-01-01 00:00:00         3 2021-01-01 03:00:00  1001       0.427  #>  5 2021-01-01 00:00:00         4 2021-01-01 04:00:00  1001       0.0798 #>  6 2021-01-01 00:00:00         5 2021-01-01 05:00:00  1001       0.762  #>  7 2021-01-01 00:00:00         6 2021-01-01 06:00:00  1001       0.347  #>  8 2021-01-01 00:00:00         7 2021-01-01 07:00:00  1001       0.488  #>  9 2021-01-01 00:00:00         8 2021-01-01 08:00:00  1001       0.438  #> 10 2021-01-01 00:00:00         9 2021-01-01 09:00:00  1001       0.662  #> # ℹ 38 more rows #> # ℹ 1 more variable: point_mbr001 <dbl>  # Select member 0 from a harp_list select_members(ens_point_list, 0) #> Members only supplied for one forecast model. Recycling members for all forecast models. #> • a #> ::ensemble point forecast:: # A tibble: 48 × 6 #>    fcst_model fcst_dttm           lead_time valid_dttm            SID a_mbr000 #>    <chr>      <dttm>                  <dbl> <dttm>              <dbl>    <dbl> #>  1 a          2021-01-01 00:00:00         0 2021-01-01 00:00:00  1001   0.461  #>  2 a          2021-01-01 00:00:00         1 2021-01-01 01:00:00  1001   0.674  #>  3 a          2021-01-01 00:00:00         2 2021-01-01 02:00:00  1001   0.944  #>  4 a          2021-01-01 00:00:00         3 2021-01-01 03:00:00  1001   0.169  #>  5 a          2021-01-01 00:00:00         4 2021-01-01 04:00:00  1001   0.729  #>  6 a          2021-01-01 00:00:00         5 2021-01-01 05:00:00  1001   0.191  #>  7 a          2021-01-01 00:00:00         6 2021-01-01 06:00:00  1001   0.266  #>  8 a          2021-01-01 00:00:00         7 2021-01-01 07:00:00  1001   0.0762 #>  9 a          2021-01-01 00:00:00         8 2021-01-01 08:00:00  1001   0.308  #> 10 a          2021-01-01 00:00:00         9 2021-01-01 09:00:00  1001   0.630  #> # ℹ 38 more rows #>  #> • b #> ::ensemble point forecast:: # A tibble: 48 × 6 #>    fcst_model fcst_dttm           lead_time valid_dttm            SID b_mbr000 #>    <chr>      <dttm>                  <dbl> <dttm>              <dbl>    <dbl> #>  1 b          2021-01-01 00:00:00         0 2021-01-01 00:00:00  1001    0.887 #>  2 b          2021-01-01 00:00:00         1 2021-01-01 01:00:00  1001    0.401 #>  3 b          2021-01-01 00:00:00         2 2021-01-01 02:00:00  1001    0.534 #>  4 b          2021-01-01 00:00:00         3 2021-01-01 03:00:00  1001    0.509 #>  5 b          2021-01-01 00:00:00         4 2021-01-01 04:00:00  1001    0.442 #>  6 b          2021-01-01 00:00:00         5 2021-01-01 05:00:00  1001    0.319 #>  7 b          2021-01-01 00:00:00         6 2021-01-01 06:00:00  1001    0.971 #>  8 b          2021-01-01 00:00:00         7 2021-01-01 07:00:00  1001    0.452 #>  9 b          2021-01-01 00:00:00         8 2021-01-01 08:00:00  1001    0.547 #> 10 b          2021-01-01 00:00:00         9 2021-01-01 09:00:00  1001    0.339 #> # ℹ 38 more rows #>   # Different members can be selected from each data frame select_members(ens_point_list, list(a = 0, b = 1)) #> • a #> ::ensemble point forecast:: # A tibble: 48 × 6 #>    fcst_model fcst_dttm           lead_time valid_dttm            SID a_mbr000 #>    <chr>      <dttm>                  <dbl> <dttm>              <dbl>    <dbl> #>  1 a          2021-01-01 00:00:00         0 2021-01-01 00:00:00  1001   0.461  #>  2 a          2021-01-01 00:00:00         1 2021-01-01 01:00:00  1001   0.674  #>  3 a          2021-01-01 00:00:00         2 2021-01-01 02:00:00  1001   0.944  #>  4 a          2021-01-01 00:00:00         3 2021-01-01 03:00:00  1001   0.169  #>  5 a          2021-01-01 00:00:00         4 2021-01-01 04:00:00  1001   0.729  #>  6 a          2021-01-01 00:00:00         5 2021-01-01 05:00:00  1001   0.191  #>  7 a          2021-01-01 00:00:00         6 2021-01-01 06:00:00  1001   0.266  #>  8 a          2021-01-01 00:00:00         7 2021-01-01 07:00:00  1001   0.0762 #>  9 a          2021-01-01 00:00:00         8 2021-01-01 08:00:00  1001   0.308  #> 10 a          2021-01-01 00:00:00         9 2021-01-01 09:00:00  1001   0.630  #> # ℹ 38 more rows #>  #> • b #> ::ensemble point forecast:: # A tibble: 48 × 6 #>    fcst_model fcst_dttm           lead_time valid_dttm            SID b_mbr001 #>    <chr>      <dttm>                  <dbl> <dttm>              <dbl>    <dbl> #>  1 b          2021-01-01 00:00:00         0 2021-01-01 00:00:00  1001   0.439  #>  2 b          2021-01-01 00:00:00         1 2021-01-01 01:00:00  1001   0.167  #>  3 b          2021-01-01 00:00:00         2 2021-01-01 02:00:00  1001   0.820  #>  4 b          2021-01-01 00:00:00         3 2021-01-01 03:00:00  1001   0.527  #>  5 b          2021-01-01 00:00:00         4 2021-01-01 04:00:00  1001   0.888  #>  6 b          2021-01-01 00:00:00         5 2021-01-01 05:00:00  1001   0.875  #>  7 b          2021-01-01 00:00:00         6 2021-01-01 06:00:00  1001   0.327  #>  8 b          2021-01-01 00:00:00         7 2021-01-01 07:00:00  1001   0.0630 #>  9 b          2021-01-01 00:00:00         8 2021-01-01 08:00:00  1001   0.755  #> 10 b          2021-01-01 00:00:00         9 2021-01-01 09:00:00  1001   0.349  #> # ℹ 38 more rows #>   # Deselect members with invert = TRUE select_members(ens_point_df, 0, invert = TRUE) #> ::ensemble point forecast:: # A tibble: 48 × 5 #>    fcst_dttm           lead_time valid_dttm            SID point_mbr001 #>    <dttm>                  <dbl> <dttm>              <dbl>        <dbl> #>  1 2021-01-01 00:00:00         0 2021-01-01 00:00:00  1001       0.789  #>  2 2021-01-01 00:00:00         1 2021-01-01 01:00:00  1001       0.544  #>  3 2021-01-01 00:00:00         2 2021-01-01 02:00:00  1001       0.454  #>  4 2021-01-01 00:00:00         3 2021-01-01 03:00:00  1001       0.677  #>  5 2021-01-01 00:00:00         4 2021-01-01 04:00:00  1001       0.532  #>  6 2021-01-01 00:00:00         5 2021-01-01 05:00:00  1001       0.107  #>  7 2021-01-01 00:00:00         6 2021-01-01 06:00:00  1001       0.407  #>  8 2021-01-01 00:00:00         7 2021-01-01 07:00:00  1001       0.0583 #>  9 2021-01-01 00:00:00         8 2021-01-01 08:00:00  1001       0.946  #> 10 2021-01-01 00:00:00         9 2021-01-01 09:00:00  1001       0.462  #> # ℹ 38 more rows select_members(ens_point_list, list(a = 0, b = 1), invert = TRUE) #> • a #> ::ensemble point forecast:: # A tibble: 48 × 6 #>    fcst_model fcst_dttm           lead_time valid_dttm            SID a_mbr001 #>    <chr>      <dttm>                  <dbl> <dttm>              <dbl>    <dbl> #>  1 a          2021-01-01 00:00:00         0 2021-01-01 00:00:00  1001    0.810 #>  2 a          2021-01-01 00:00:00         1 2021-01-01 01:00:00  1001    0.452 #>  3 a          2021-01-01 00:00:00         2 2021-01-01 02:00:00  1001    0.853 #>  4 a          2021-01-01 00:00:00         3 2021-01-01 03:00:00  1001    0.467 #>  5 a          2021-01-01 00:00:00         4 2021-01-01 04:00:00  1001    0.569 #>  6 a          2021-01-01 00:00:00         5 2021-01-01 05:00:00  1001    0.876 #>  7 a          2021-01-01 00:00:00         6 2021-01-01 06:00:00  1001    0.301 #>  8 a          2021-01-01 00:00:00         7 2021-01-01 07:00:00  1001    0.598 #>  9 a          2021-01-01 00:00:00         8 2021-01-01 08:00:00  1001    0.803 #> 10 a          2021-01-01 00:00:00         9 2021-01-01 09:00:00  1001    0.595 #> # ℹ 38 more rows #>  #> • b #> ::ensemble point forecast:: # A tibble: 48 × 6 #>    fcst_model fcst_dttm           lead_time valid_dttm            SID b_mbr000 #>    <chr>      <dttm>                  <dbl> <dttm>              <dbl>    <dbl> #>  1 b          2021-01-01 00:00:00         0 2021-01-01 00:00:00  1001    0.887 #>  2 b          2021-01-01 00:00:00         1 2021-01-01 01:00:00  1001    0.401 #>  3 b          2021-01-01 00:00:00         2 2021-01-01 02:00:00  1001    0.534 #>  4 b          2021-01-01 00:00:00         3 2021-01-01 03:00:00  1001    0.509 #>  5 b          2021-01-01 00:00:00         4 2021-01-01 04:00:00  1001    0.442 #>  6 b          2021-01-01 00:00:00         5 2021-01-01 05:00:00  1001    0.319 #>  7 b          2021-01-01 00:00:00         6 2021-01-01 06:00:00  1001    0.971 #>  8 b          2021-01-01 00:00:00         7 2021-01-01 07:00:00  1001    0.452 #>  9 b          2021-01-01 00:00:00         8 2021-01-01 08:00:00  1001    0.547 #> 10 b          2021-01-01 00:00:00         9 2021-01-01 09:00:00  1001    0.339 #> # ℹ 38 more rows #>"},{"path":"/reference/seq_double.html","id":null,"dir":"Reference","previous_headings":"","what":"Generate a doubled sequence — seq_double","title":"Generate a doubled sequence — seq_double","text":"seq_double generates sequence numbers value double previous value sequence.","code":""},{"path":"/reference/seq_double.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Generate a doubled sequence — seq_double","text":"","code":"seq_double(start, len)"},{"path":"/reference/seq_double.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Generate a doubled sequence — seq_double","text":"start first number sequence. len length sequence.","code":""},{"path":"/reference/seq_double.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Generate a doubled sequence — seq_double","text":"numeric vector","code":""},{"path":"/reference/seq_double.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Generate a doubled sequence — seq_double","text":"","code":"seq_double(1, 5) #> [1]  1  2  4  8 16 seq_double(0.125, 10) #>  [1]  0.125  0.250  0.500  1.000  2.000  4.000  8.000 16.000 32.000 64.000"},{"path":"/reference/seq_dttm.html","id":null,"dir":"Reference","previous_headings":"","what":"Generate a sequence of date-time strings — seq_dttm","title":"Generate a sequence of date-time strings — seq_dttm","text":"Given start date-time, end date-time time resolution regular sequence date-time strings generated. start end date-times must string numeric form YYYYMMDD, YYYYMMDDhh, YYYYMMDDhhmm, YYYYMMDDhhmmss.","code":""},{"path":"/reference/seq_dttm.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Generate a sequence of date-time strings — seq_dttm","text":"","code":"seq_dttm(start_dttm, end_dttm, by = \"1h\")"},{"path":"/reference/seq_dttm.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Generate a sequence of date-time strings — seq_dttm","text":"start_dttm date-time start sequence. Must string numeric form YYYYMMDD, YYYYMMDDhh, YYYYMMDDhhmm, YYYYMMDDhhmmss. end_dttm date-time end sequence. Must string numeric form YYYYMMDD, YYYYMMDDhh, YYYYMMDDhhmm, YYYYMMDDhhmmss. Increment sequence. numeric, considered hours, otherwise string number followed unit. Units can \"s\", seconds; \"m\", minutes; \"h\", hours; \"d\", days.","code":""},{"path":"/reference/seq_dttm.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Generate a sequence of date-time strings — seq_dttm","text":"sequence date-time strings","code":""},{"path":"/reference/seq_dttm.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Generate a sequence of date-time strings — seq_dttm","text":"output sequence vector strings. Truncation strings done last zero values removed.","code":""},{"path":"/reference/seq_dttm.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Generate a sequence of date-time strings — seq_dttm","text":"","code":"seq_dttm(20220306, 20220307) #>  [1] \"2022030600\" \"2022030601\" \"2022030602\" \"2022030603\" \"2022030604\" #>  [6] \"2022030605\" \"2022030606\" \"2022030607\" \"2022030608\" \"2022030609\" #> [11] \"2022030610\" \"2022030611\" \"2022030612\" \"2022030613\" \"2022030614\" #> [16] \"2022030615\" \"2022030616\" \"2022030617\" \"2022030618\" \"2022030619\" #> [21] \"2022030620\" \"2022030621\" \"2022030622\" \"2022030623\" \"2022030700\" seq_dttm(20220306, 20220307, by = \"30m\") #>  [1] \"202203060000\" \"202203060030\" \"202203060100\" \"202203060130\" \"202203060200\" #>  [6] \"202203060230\" \"202203060300\" \"202203060330\" \"202203060400\" \"202203060430\" #> [11] \"202203060500\" \"202203060530\" \"202203060600\" \"202203060630\" \"202203060700\" #> [16] \"202203060730\" \"202203060800\" \"202203060830\" \"202203060900\" \"202203060930\" #> [21] \"202203061000\" \"202203061030\" \"202203061100\" \"202203061130\" \"202203061200\" #> [26] \"202203061230\" \"202203061300\" \"202203061330\" \"202203061400\" \"202203061430\" #> [31] \"202203061500\" \"202203061530\" \"202203061600\" \"202203061630\" \"202203061700\" #> [36] \"202203061730\" \"202203061800\" \"202203061830\" \"202203061900\" \"202203061930\" #> [41] \"202203062000\" \"202203062030\" \"202203062100\" \"202203062130\" \"202203062200\" #> [46] \"202203062230\" \"202203062300\" \"202203062330\" \"202203070000\" seq_dttm(20220301, 20220331, by = \"1d\") #>  [1] \"20220301\" \"20220302\" \"20220303\" \"20220304\" \"20220305\" \"20220306\" #>  [7] \"20220307\" \"20220308\" \"20220309\" \"20220310\" \"20220311\" \"20220312\" #> [13] \"20220313\" \"20220314\" \"20220315\" \"20220316\" \"20220317\" \"20220318\" #> [19] \"20220319\" \"20220320\" \"20220321\" \"20220322\" \"20220323\" \"20220324\" #> [25] \"20220325\" \"20220326\" \"20220327\" \"20220328\" \"20220329\" \"20220330\" #> [31] \"20220331\" seq_dttm(202203061030, 202203061045, by = \"30s\") #>  [1] \"20220306103000\" \"20220306103030\" \"20220306103100\" \"20220306103130\" #>  [5] \"20220306103200\" \"20220306103230\" \"20220306103300\" \"20220306103330\" #>  [9] \"20220306103400\" \"20220306103430\" \"20220306103500\" \"20220306103530\" #> [13] \"20220306103600\" \"20220306103630\" \"20220306103700\" \"20220306103730\" #> [17] \"20220306103800\" \"20220306103830\" \"20220306103900\" \"20220306103930\" #> [21] \"20220306104000\" \"20220306104030\" \"20220306104100\" \"20220306104130\" #> [25] \"20220306104200\" \"20220306104230\" \"20220306104300\" \"20220306104330\" #> [29] \"20220306104400\" \"20220306104430\" \"20220306104500\""},{"path":"/reference/seq_pwr2.html","id":null,"dir":"Reference","previous_headings":"","what":"Generate a sequence of integer powers of 2 — seq_pwr2","title":"Generate a sequence of integer powers of 2 — seq_pwr2","text":"sequence powers 2 generated based first value length desired sequence. first value either first power sequence, first value sequence. powers argument used tell function first value .","code":""},{"path":"/reference/seq_pwr2.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Generate a sequence of integer powers of 2 — seq_pwr2","text":"","code":"seq_pwr2(from, len, powers = TRUE)"},{"path":"/reference/seq_pwr2.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Generate a sequence of integer powers of 2 — seq_pwr2","text":"starting power value first number sequence. case latter, must value 2 raised integer power. len length desired sequence. powers Logical. Whether first argument first power begin sequence , whether value begin sequence .","code":""},{"path":"/reference/seq_pwr2.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Generate a sequence of integer powers of 2 — seq_pwr2","text":"numeric vector consecutive powers 2.","code":""},{"path":"/reference/seq_pwr2.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Generate a sequence of integer powers of 2 — seq_pwr2","text":"function generates sequences 2 raised integer powers. can useful generating breaks values exponential colour scales.","code":""},{"path":"/reference/seq_pwr2.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Generate a sequence of integer powers of 2 — seq_pwr2","text":"","code":"# Start from 1 seq_pwr2(0, 5) #> [1]  1  2  4  8 16 seq_pwr2(1, 5, powers = FALSE) #> [1]  1  2  4  8 16 # Start from a value larger than 1 seq_pwr2(3, 5) #> [1]   8  16  32  64 128 seq_pwr2(8, 5, powers = FALSE) #> [1]   8  16  32  64 128 # Start from a value smaller than 1 seq_pwr2(-3, 5) #> [1] 0.125 0.250 0.500 1.000 2.000 seq_pwr2(0.125, 5, powers = FALSE) #> [1] 0.125 0.250 0.500 1.000 2.000"},{"path":"/reference/set_units.html","id":null,"dir":"Reference","previous_headings":"","what":"Add or modify a units column — set_units","title":"Add or modify a units column — set_units","text":"Add modify units column","code":""},{"path":"/reference/set_units.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Add or modify a units column — set_units","text":"","code":"set_units(x, units)"},{"path":"/reference/set_units.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Add or modify a units column — set_units","text":"x data frame harp_list. units units name put units column.","code":""},{"path":"/reference/set_units.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Add or modify a units column — set_units","text":"data frame harp_list size x units column either added modified.","code":""},{"path":"/reference/set_units.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Add or modify a units column — set_units","text":"","code":"# det_point_df has no units column det_point_df #> ::deterministic point forecast:: # A tibble: 48 × 6 #>    fcst_model fcst_dttm           lead_time valid_dttm            SID  fcst #>  * <chr>      <dttm>                  <dbl> <dttm>              <dbl> <dbl> #>  1 point      2021-01-01 00:00:00         0 2021-01-01 00:00:00  1001 0.300 #>  2 point      2021-01-01 00:00:00         1 2021-01-01 01:00:00  1001 0.611 #>  3 point      2021-01-01 00:00:00         2 2021-01-01 02:00:00  1001 0.802 #>  4 point      2021-01-01 00:00:00         3 2021-01-01 03:00:00  1001 0.361 #>  5 point      2021-01-01 00:00:00         4 2021-01-01 04:00:00  1001 0.213 #>  6 point      2021-01-01 00:00:00         5 2021-01-01 05:00:00  1001 0.736 #>  7 point      2021-01-01 00:00:00         6 2021-01-01 06:00:00  1001 0.177 #>  8 point      2021-01-01 00:00:00         7 2021-01-01 07:00:00  1001 0.866 #>  9 point      2021-01-01 00:00:00         8 2021-01-01 08:00:00  1001 0.109 #> 10 point      2021-01-01 00:00:00         9 2021-01-01 09:00:00  1001 0.436 #> # ℹ 38 more rows  # Set units to \"degC\" new_det_point_df <- set_units(det_point_df, \"degC\") new_det_point_df #> ::deterministic point forecast:: # A tibble: 48 × 7 #>    fcst_model fcst_dttm           lead_time valid_dttm            SID  fcst #>    <chr>      <dttm>                  <dbl> <dttm>              <dbl> <dbl> #>  1 point      2021-01-01 00:00:00         0 2021-01-01 00:00:00  1001 0.300 #>  2 point      2021-01-01 00:00:00         1 2021-01-01 01:00:00  1001 0.611 #>  3 point      2021-01-01 00:00:00         2 2021-01-01 02:00:00  1001 0.802 #>  4 point      2021-01-01 00:00:00         3 2021-01-01 03:00:00  1001 0.361 #>  5 point      2021-01-01 00:00:00         4 2021-01-01 04:00:00  1001 0.213 #>  6 point      2021-01-01 00:00:00         5 2021-01-01 05:00:00  1001 0.736 #>  7 point      2021-01-01 00:00:00         6 2021-01-01 06:00:00  1001 0.177 #>  8 point      2021-01-01 00:00:00         7 2021-01-01 07:00:00  1001 0.866 #>  9 point      2021-01-01 00:00:00         8 2021-01-01 08:00:00  1001 0.109 #> 10 point      2021-01-01 00:00:00         9 2021-01-01 09:00:00  1001 0.436 #> # ℹ 38 more rows #> # ℹ 1 more variable: units <chr>  # Modify the units name to \"degrees_C\" set_units(new_det_point_df, \"degrees_C\") #> ::deterministic point forecast:: # A tibble: 48 × 7 #>    fcst_model fcst_dttm           lead_time valid_dttm            SID  fcst #>    <chr>      <dttm>                  <dbl> <dttm>              <dbl> <dbl> #>  1 point      2021-01-01 00:00:00         0 2021-01-01 00:00:00  1001 0.300 #>  2 point      2021-01-01 00:00:00         1 2021-01-01 01:00:00  1001 0.611 #>  3 point      2021-01-01 00:00:00         2 2021-01-01 02:00:00  1001 0.802 #>  4 point      2021-01-01 00:00:00         3 2021-01-01 03:00:00  1001 0.361 #>  5 point      2021-01-01 00:00:00         4 2021-01-01 04:00:00  1001 0.213 #>  6 point      2021-01-01 00:00:00         5 2021-01-01 05:00:00  1001 0.736 #>  7 point      2021-01-01 00:00:00         6 2021-01-01 06:00:00  1001 0.177 #>  8 point      2021-01-01 00:00:00         7 2021-01-01 07:00:00  1001 0.866 #>  9 point      2021-01-01 00:00:00         8 2021-01-01 08:00:00  1001 0.109 #> 10 point      2021-01-01 00:00:00         9 2021-01-01 09:00:00  1001 0.436 #> # ℹ 38 more rows #> # ℹ 1 more variable: units <chr>"},{"path":"/reference/station_groups.html","id":null,"dir":"Reference","previous_headings":"","what":"Weather station IDs with regional groups — station_groups","title":"Weather station IDs with regional groups — station_groups","text":"dataset station IDs geographic groups belong . Can used grouped verification joining station_groups harp_fcst list running verification groupings = c(\"leadtime\", \"group\"). Note many stations belong one group","code":""},{"path":"/reference/station_groups.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Weather station IDs with regional groups — station_groups","text":"","code":"station_groups"},{"path":"/reference/station_groups.html","id":"format","dir":"Reference","previous_headings":"","what":"Format","title":"Weather station IDs with regional groups — station_groups","text":"data frame 6001 rows 2 variables: SID station ID number station_group Geographic group station belongs ","code":""},{"path":"/reference/station_groups.html","id":"source","dir":"Reference","previous_headings":"","what":"Source","title":"Weather station IDs with regional groups — station_groups","text":"selection.pm HARMONIE / HIRLAM monitor","code":""},{"path":"/reference/station_list.html","id":null,"dir":"Reference","previous_headings":"","what":"List of Weather Stations — station_list","title":"List of Weather Stations — station_list","text":"dataset containing site ID, latitude, longitude, elevation name 13417 weather stations world wide.","code":""},{"path":"/reference/station_list.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"List of Weather Stations — station_list","text":"","code":"station_list"},{"path":"/reference/station_list.html","id":"format","dir":"Reference","previous_headings":"","what":"Format","title":"List of Weather Stations — station_list","text":"data frame 13417 rows 5 variables: SID station ID number lat latitude station, decimal degrees lon longitude station, decimal degrees elev elevation station, metres name name station","code":""},{"path":"/reference/station_list.html","id":"source","dir":"Reference","previous_headings":"","what":"Source","title":"List of Weather Stations — station_list","text":"HIRLAM station list","code":""},{"path":"/reference/to_seconds.html","id":null,"dir":"Reference","previous_headings":"","what":"Convert a time period to seconds — to_seconds","title":"Convert a time period to seconds — to_seconds","text":"Convert number minutes, hours, days weeks seconds. x numeric, assumed hours.","code":""},{"path":"/reference/to_seconds.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Convert a time period to seconds — to_seconds","text":"","code":"to_seconds(x)"},{"path":"/reference/to_seconds.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Convert a time period to seconds — to_seconds","text":"x character numeric vector. units input specified single character: s seconds m minutes h hours d days w weeks","code":""},{"path":"/reference/to_seconds.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Convert a time period to seconds — to_seconds","text":"named vector values number seconds names equivalent units given input","code":""},{"path":"/reference/to_seconds.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Convert a time period to seconds — to_seconds","text":"","code":"# Numeric inputs are assumed to be in hours to_seconds(c(0, 1, 2)) #>   0h   1h   2h  #>    0 3600 7200   # The same values given in seconds, minutes and explicitly hours to_seconds(c(\"0s\", \"3600s\", \"7200s\")) #>    0s 3600s 7200s  #>     0  3600  7200  to_seconds(c(\"0m\", \"60m\", \"120m\")) #>   0m  60m 120m  #>    0 3600 7200  to_seconds(c(\"0h\", \"1h\", \"2h\")) #>   0h   1h   2h  #>    0 3600 7200   # Units can be mixed to_seconds(c(paste0(24 * 7, \"h\"), \"7d\", \"1w\")) #>   168h     7d     1w  #> 604800 604800 604800"},{"path":"/reference/unique_col.html","id":null,"dir":"Reference","previous_headings":"","what":"Extract unique values from a data frame column — unique_col","title":"Extract unique values from a data frame column — unique_col","text":"Unique values extracted named column data frame. case harp_list, unique values across data frames list extracted single vector. column found, warning issued NULL returned.","code":""},{"path":"/reference/unique_col.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Extract unique values from a data frame column — unique_col","text":"","code":"unique_col(.data, col)  unique_stations(.data)  unique_valid_dttm(.data)  unique_fcst_dttm(.data)"},{"path":"/reference/unique_col.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Extract unique values from a data frame column — unique_col","text":".data data frame harp_list col column extract unique values. Can quoted unquoted.","code":""},{"path":"/reference/unique_col.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Extract unique values from a data frame column — unique_col","text":"vector unique values.","code":""},{"path":"/reference/unique_col.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Extract unique values from a data frame column — unique_col","text":"unique_stations, unique_fcst_dttm unique_valid_dttm wrappers around unique_col extract unique stations, valid date-time forecast start date-time using standard harp column names \"SID\", \"valid_dttm\" \"fcst_dttm\" respectively.","code":""},{"path":"/reference/unique_col.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Extract unique values from a data frame column — unique_col","text":"","code":"unique_col(det_point_df, fcst_dttm) #> [1] \"2021-01-01 UTC\" unique_col(det_point_df, SID) #> [1] 1001 1002  # Works with quoted column names too unique_col(det_point_df, \"SID\") #> [1] 1001 1002  # Use {{<var>}} for variables as columns my_col <- \"SID\" unique_col(det_point_df, {{my_col}}) #> [1] 1001 1002"},{"path":"/reference/unixtime_to_dttm.html","id":null,"dir":"Reference","previous_headings":"","what":"Convert unix time to other formats. — unixtime_to_dttm","title":"Convert unix time to other formats. — unixtime_to_dttm","text":"unix time expected seconds since 1970-01-01 00:00:00 UTC time zone.","code":""},{"path":"/reference/unixtime_to_dttm.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Convert unix time to other formats. — unixtime_to_dttm","text":"","code":"unixtime_to_dttm(x)  unixtime_to_YMD(x)  unixtime_to_YMDh(x)  unixtime_to_YMDhm(x)  unixtime_to_YMDhms(x)  unixtime_to_ymd(x)  unixtime_to_ymdh(x)  unixtime_to_ymdhm(x)  unixtime_to_ymdhms(x)  unixtime_to_str_dttm(x)"},{"path":"/reference/unixtime_to_dttm.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Convert unix time to other formats. — unixtime_to_dttm","text":"x numeric vector UNIX time seconds since 1970-01-01 00:00:00.","code":""},{"path":"/reference/unixtime_to_dttm.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Convert unix time to other formats. — unixtime_to_dttm","text":"date-time object character date-time string desired format","code":""},{"path":"/reference/unixtime_to_dttm.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Convert unix time to other formats. — unixtime_to_dttm","text":"","code":"unixtime_to_dttm(Sys.time()) #> [1] \"2023-11-16 10:18:04 UTC\" unixtime_to_ymd(Sys.time()) #> [1] \"20231116\" unixtime_to_ymdh(Sys.time()) #> [1] \"2023111610\" unixtime_to_ymdhm(Sys.time()) #> [1] \"202311161018\" unixtime_to_ymdhms(Sys.time()) #> [1] \"20231116101804\" unixtime_to_str_dttm(Sys.time()) #> [1] \"20231116101804\""},{"path":"/news/index.html","id":"harpcore-020","dir":"Changelog","previous_headings":"","what":"harpCore 0.2.0","title":"harpCore 0.2.0","text":"Initial version. Tagged v0.2.0 consistency versioning existing harp packages","code":""}]
